\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
%\usepackage[spanish, mexico]{babel}
\usepackage{listings}
\usepackage{breakcites}
\usepackage{dsfont}
\usepackage{hyperref}
\usepackage{amssymb,amsthm,amsmath,latexsym}
\usepackage[margin=1.5cm]{geometry}
\usepackage{natbib}
%\bibliographystyle{stylename}
%\usepackage{fancyhdr}
%\pagestyle{fancy}
\theoremstyle{plain}
\newtheorem{teo}{Theorem}
\newtheorem{prop}[teo]{Proposition}
\newtheorem{defi}[teo]{Definition}
\newtheorem{obs}[teo]{Observation}
\newtheorem{lem}[teo]{Lemma}
\newtheorem{cor}[teo]{Corolary}
\usepackage[pdftex]{color,graphicx}
\usepackage{tikz}
\usetikzlibrary{trees}
\usetikzlibrary{calc}
%\newcommand{\sgn}{\mathop{\mathrm{sgn}}}
\title{Use and acquisition of causal relations for decision making under uncertainty using imperfect information repeated games}
\author{Mauricio Gonzalez Soto}
\begin{document}
%\nocite{*}
\maketitle
\tableofcontents
\newpage
\begin{abstract}
We consider decision problems under uncertainty where the options available to a decision maker and the resulting outcome are related through a causal mechanism which is unknown to the decision maker. We study how a decision maker can learn about this causal mechanism through sequential decision making as well as using current causal knowledge inside each round in order to make better choices had she not considered causal knowledge. We propose a decision making procedure in which an agent holds \textit{beliefs} about her environment which are used to make a choice and then are updated using the observed outcome. As proof of concept, we present an implementation of this causal decision making model and apply it to a simple problem. We show that the model achieves a performance similar to the classic Q-learning while it also acquires a causal model of the environment. 
\end{abstract}
\section{Introduction}
\indent A fundamental part of intelligent reasoning is being able to make decisions under uncertain conditions (\cite{danks2014unifying}, \cite{lake2017building}, \cite{pearlwhy}). In some cases, a decision maker who faces an uncertain environment has enough information to make choices by maximizing expected utility, which is the classic formal criteria for making decisions if rational preferences are assumed (\cite{bernardo2000bayesian}, \cite{gilboa2009decision}). On the other hand, if enough information is not available, the decision maker could attempt to \textit{learn} from the environment by interacting with it.

Learning by interaction has been extensively studied by computer scientists using the Reinforcement Learning (RL) setting \cite{sutton1998reinforcement}, but the most common used techniques  in this field are purely associative and do not consider any high-level structure of the environment beyond what is expressable in a Markov Decision Process \cite{garnelo2016towards}.

Since human beings are known to learn causal models in sequential decision making processes (\cite{sloman2006causal}, \cite{nichols2007decision}, \cite{meder2010observing}, \cite{hagmayer2013repeated}, \cite{danks2014unifying}), and even though this learning is not perfect \cite{rottman2014reasoning}, we propose that an autonomous agent can learn and use causal information while interacting with an uncertain environment which is governed by a fixed \textit{causal mechanism} which is unknown to the agent.  

The proposed way for an agent to learn from repeated interactions is by giving her \textit{beliefs} about the structure of the environment and a way to update them after an outcome has been observed.

While the standard setting in RL is to model the agent-environment interaction as an agent that moves from one \textit{state} to another inside a model of the environment and observing a reward as these transitions occur, we propose to model it as a \textit{game} between the decision maker and a player called \textit{Nature} which will select his actions from the causal model in response to what the decision maker has chosen. 

The agent, using her current beliefs, will generate a \textit{local} causal model and choose an action from it as if that model was the true one. Then, after she observes the consequences of her actions, her beliefs will be updated according to the observed information in order to make a better choice the next time. The agent, besides learning a policy to choose actions will also learn a causal model from the environment since the causal model she forms will approximate the true model.

Learning a causal model of the environment allows to extract high-level insights of a phenomena beyond associative descriptions of what is observed.

\section{Main Concepts}
	\subsection{Attempts to define Causality}
	Causality has been a difficult concept to define and several attempts have been made. In fact, in mainstream statistics  the term \textit{cause} have been nearly banned as documented by \cite{pearl2018why}.
	
	In the classic works on Causality (\cite{spirtes2000causation}, \cite{pearl2009causality}) the term \textit{causality} is defined in terms of common language and then the mathematical machinery required to model causality is provided. In this way, causality becomes defined as what is modeled by causal models, which are themselves defined as to model causality. This apparent circularity is deeply studied by \cite{woodward2005making}. For this reason, we will give here a formal working definition of causal relations which allows to be used formally.
	\subsection{Definition of Causality}
	Causality is understood as a relation between \textit{events}. Some event \textit{causes} another event to occurr. The formal definition of Causality that will be adopted in this work is the definition provided in \cite{spirtes2000causation}.
	\begin{defi}{\label{causal_relation}}
	Let $(\Omega, \mathcal{F}, \mathbb{P})$ a finite probability space, and consider a binary relation $\to \subseteq \mathcal{F} \times \mathcal{F}$ which is:
	\begin{itemize}
	\item Transitivity: If $A \to B$ and $B \to C$ for any $A, B, C \in \mathcal{F}$ then $A \to C$.
	\item Irreflexive: For all $A \in \mathcal{F}$ it doesn't hold that $A \to A$.
	\item Antisymmetric: For $A,B \in \mathcal{F}$ such that $A \neq B$ if $A \to B$ then it doesn't hold that $B \to A$.
	\end{itemize}
	We will say that $A$ is \textit{a cause} of $B$ (or that $A$ causes $B$, $A$ is the cause and $B$ is the effect) if $A \to B$.
	\end{defi}
	We will assume that for any event $A \in \mathcal{F}$ there are no causes lying outside $\mathcal{F}$, and that the relations expressed by $\to$ are the only causal relations in the environment. This assumption can be interpreted as not allowing \textit{intermediate} events between events known to be causally related. For example, if in our space we have that striking a match causes fire, we will not allow into consideration the underlying chemical reactions that cause fire from friction. In this sense, we will say that striking a match is a \textit{direct cause} of fire and this will be the only causes considered (\cite{spirtes2000causation}). 
	\subsection{Representation into a Directed Acyclic Graph}
	Making some notation abuse, the causal relations contained in $\to$ can be summarized in a graph $G=(V,E)$ in the following way: If $A \to B$ then the graph must contain a node $A \in V$ representing $A$, a node  $B \in V$ representing $B$ and a directed edge $e \in E$ connecting the respective nodes in the direction of the causal relation.
	\begin{prop}
	Given a causal relation $\to$ as in Definition \ref{causal_relation} then the graph that is obtained by considering nodes for events and edges for the causal relations as previously described is a Directed Acyclic Graph.
	\end{prop}
	\begin{proof}
	The graph is directed since the definition of causality imposes a direction between events; namely, a direction between the cause and the effect. To see that the graph is acyclic, suppose that a cycle $A \to B \to C \to A$ exists, this would imply, because of transitivity, that $A \to A$, which can not be since the relation is irreflexive.
	\end{proof}
	Notice that since the graph is finite, there exist some nodes that does not have causes, which are called \textit{exogenous}. If an $A$ event is caused by some other event, then we say it is \textit{endogenous} and we denote the set of its causes as $Pa(A)$.
	\subsection{Causal Graphical Models}
	\subsection{Decision Theory}
	A Decision Problem under Uncertainty is a situation in which an agent must choose one out of many actions with uncertain consequences. The most common theories for Decision Making under Uncertainty are those from \cite{von1944theory} and \cite{savage1954the}. In the former theory it is assumed that the decision maker knows the stochastic relation between actions and consequences, and in that case the theory guarantees that the decision maker behaves as if he maximizes the expected value of an utility function. On the other hand, if the decision maker doesn't know the probabilities of observing outcomes given a chosen action, then Savage's theory guarantees that the decision maker behaves as if he had in mind a \textit{subjective} probability distribution, a utility function and chooses the action which maximizes the expected utility with respect to that subjective probability distribution and utility function. 
	
Both von Neumann and Savage's Decision Under Uncertainty theories are known as the \textit{classic} theories in other areas, mostly in Economics. It is important to say that other Decision Making theories exist, such as Prospect Theory (\cite{kahneman1979prospect}), Case-Based Decision Theory (\cite{gilboa1995case}) among others that are out of the scope of this work.

For what remains of this section, we follow \cite{bernardo2000bayesian} in his exposition of Savage's theory since it is the most suitable for our purposes because we will be considering decision makers that do not know all of their environment.
	
	\subsubsection{Elements of a Decision Problem}
	A Decision Problem under Uncertainty is composed by:
	\begin{itemize}
	\item A set $\Omega$ of \textit{states}.
	\item A set $\mathcal{A}=\{a_i : i \in I \}$ of actions.
	\item For each action $a_i$, a partition $E_i = \{ E_j : j \in J \}$ of $\Omega$. Define $\mathcal{E} = \cup_{i \in I } E_i$.
	\item For each action $a_i$, a set of consequences $C_i = \{ c_j : j \in J \}$. Define $\mathcal{C}=\cup_{i \in I} C_i$
	\item A preference relation $\succeq$ defined over $\mathcal{A}$ which represent the decision-maker whishes. It is assumed that the decision maker will choose among his options according to his preferences, which are represented by $\succeq$.
	\end{itemize}
	Since we are considering that a decision maker chooses an action $a \in \mathcal{A}$, and then an uncertain event $E$ will occur which will then produce the consequence $c$, we will identify actions in $\mathcal{A}$ as $a_i = \{ c_j | E_j : j \in J \}$. As a technical assumption required for all of the math to work, we require that elements of $\mathcal{C}$ belong to $\mathcal{A}$. This is achieved by adding into $A$ elements of the form $\{ c | \Omega \}$ for $c \in \mathcal{C}$. This being said, we can extend the preference relation $\succeq$ to elements of $\mathcal{C}$ and therefore indistinctively write $c_1 \succeq c_2$ for $c_1, c_2 \in \mathcal{C}$ or $a_1 \succeq a_2$ for $a_1,a_2 \in \mathcal{A}$. We require that $\mathcal{E}$ is an \textit{algebra}.	
	\begin{defi}
	From the prerefence relation $\succeq$ we can derive some other relations between actions:
	\begin{itemize}
	\item $a_1 \sim a_2 \Leftrightarrow a_1 \succeq a_2 \textrm{ and } a_2 \succeq a_1$
	\item $a_1 \succ a_2 \Leftrightarrow a_1 \succeq a_2 \textrm{ and it does not hold that } a_2 \succeq a_1$.
	\end{itemize}
	\end{defi}
	\begin{defi}
	A relation between events can be derived from $\succeq$ in the following way:
	\[E \succeq F \Leftrightarrow \textrm{ for all } c_1 \succeq c_2 \textrm{ it holds that } \{ c_2 | E, c_1 | E^c \} \succeq \{ c_2 | F, c_1 | F^c \}. \]
	In this case, we say that $F$ is not \textit{more likely} than $E$. It can easily be shown that $\Omega \succ \emptyset$, and a relation $\sim$ for events is defined in an analog way than for consequences.
	\end{defi}
	\begin{defi}
	Given some $G \succ \emptyset$ we define a conditional preference relation as follows:
	\begin{itemize}
	\item i) $a_1 \succeq_G a_2 \Leftrightarrow \textrm{ for all a } \{ a_1 | G, a | G^c \} \succeq \{ a_2 | G, a | G^c \}$.
	\item ii) $E \succeq_G F \Leftrightarrow \textrm{ for } c_1 \succeq_G c_2 \{ c_2 | E , c_1 | E^c \} \succeq_G \{c_2 | F , c_1  | F^c \}$ 
	\end{itemize}
	\end{defi}
	\begin{defi}
	Two events $E,F \in \mathcal{E}$ are said to be independent if and only if:
	\begin{itemize}
	\item 	
	\end{itemize}
	\end{defi}
	\subsubsection{Axioms of Coherence}
	We require the following axioms for the preference relation $\succeq$ which we will call Axioms of Coherence or Rationality, and define a Rational decision maker as a decision maker who will choose according to a preference relation and whose preferences are rational in the sense of the axioms (\cite{bernardo2000bayesian}).\\
	\\
	\indent \textbf{Axiom 1}.
	\begin{itemize}
	\item i) There exists consequences $c_1$, $c_2$ such that $c_1 \succ c_2$.
	\item ii) For all consequences $c_1, c_2$ and events $E,F \in \mathcal{E}$ either $\{  c_2 | E , c_1 | E^c\} \succeq \{ c_2 | F, c_1 | F^c \}$ or $\{ c_2 | F, c_1 | F^c \} \succeq \{  c_2 | E , c_1 | E^c\}$
	\end{itemize}
	
	\textbf{Axiom 2}.
	\begin{itemize}
	\item i) $a \succeq a$ for all $a \in \mathcal{A}$.
	\item ii) If $a \succeq b$ and $b \succeq c$ for $a,b,c \in \mathcal{A}$, then $a \succeq c$.
	\end{itemize}
	
	\textbf{Axiom 3}
	\begin{itemize}
	\item i) If $a_1 \succeq a_2$ then for all $G \succ \emptyset$ $a_1 \succeq_G a_2$.
	\end{itemize}
	
	\textbf{Axiom 4.}\\
	There exists a sub-algebra $\mathcal{S}$ of $\mathcal{E}$ and a function $\mu : \mathcal{S} \to [0,1]$ such that:
	\begin{itemize}
	\item i) $S_1 \succeq S_2$ if and only if $\mu(S_1) \succeq \mu(S_2)$.
	\item ii) If $S_1 \cap S_2 = \empty$ then $\mu(S_1 \cup S_2) = \mu(S_1) + \mu(S_2)$.
	\item iii) For any number $\alpha \in [0,1]$ and independent events $E,F$, there is an event $S$, which is called a standard event, such that $\mu(S)=\alpha$ and $S$ is independent from $E$ and from $F$.
	\item iv) If $S_1$ is independent from $S_2$ then $\mu(S_1 \cap S_2)=\mu(S_1)\mu(S_2)$.
	\item v) If $E$ independent of $S$, $S$ independent from $F$ and $E$ independent from $F$ then $E \sim S$ implies that $E \sim_F S$.
	\end{itemize}
	
	\textbf{Axiom 5.}
	\begin{itemize}
	\item i) If $c_1 \succeq c \succeq c_2$ then there exists a standard event $S$ such that $c \sim \{ c_2 | S, c_1 | S^c \}$.
	\item ii) For each event $E$ there exists a standard event $S$ such that $E \sim S$.
	\end{itemize}
	
	For interpretations of the axioms in diverse contexts we refer the reader to \cite{bernardo2000bayesian}, \cite{binmore2008rational}, \cite{gilboa2009decision}, \cite{wakker2010prospect}.
	\subsubsection{Subjective Probability and Utility}
	\begin{defi}
	Given a preference relation $\succeq$, we define the (subjective) \textbf{probability} of an event $E \in \mathcal{E}$ as the real number $\mu(S)$ associated with the standard event $S$ such that $E \sim S$.
	\end{defi}
	The subjective probability thus defined satisfies all of the Kolmogorov's axioms of probability (\cite{bernardo2000bayesian}). So, we can use all of the mathematical machinery for classic probability measures, including the definition of conditional probability, Bayes' theorem, etc.
	\begin{defi}
	Consequences $c^\ast$ and $c_\ast$ are called respectively best and worst (extreme consequences) if for any other consequence $c \in \mathcal{C}$ then $c^\ast \succeq c \succeq c_\ast$. Decision problems in which we add extreme consequences are called bounded decision problems.
	\end{defi}
	\begin{defi}
	Given a preference relation $\succeq$ we define the canonical \textbf{utility} $u(c)=u(c | c_\ast, c^\ast)$ of a consequence $c \in \mathcal{C}$ relative to the extreme consequences $c_\ast$, $c^\ast$, as the real number $\mu(S)$ associated with any standard event $S$ such that $c \sim \{c^\ast | S, c_\ast | S^c\}$.
	\end{defi}
	\subsubsection{Decision Criteria for Decision Problems}
	\begin{prop}
	For any bounded decision problem with extreme consequences $c_\ast$ and $c^\ast$ it holds:
	\begin{itemize}
	\item i) For all $c \in \mathcal{C}$, $u(c | c_\ast, c^\ast)$ exists and is unique.
	\item ii) The value of $u(c | c_\ast, c^\ast)$ is unafected by the occurrence of any event $G \succ \emptyset$.
	\item iii) $0 = u(c_\ast | c_\ast, c^\ast) \leq u(c | c_\ast, c^\ast) \leq u(c^\ast | c_\ast, c^\ast) = 1$.
	\end{itemize}
	\end{prop}
	\begin{defi}
	For a bounded decision problem and any event $G \succ \emptyset$ and $a = \{ c_j | E_j : j \in J \}$ we define the conditional expected utility of $a$ as
	\[ \bar{u}(a | c_\ast, c^\ast, G) = \sum_{j \in J} u(c_j | c_\ast, c^\ast)P(E_j | G). \]
	If $G = \Omega$ we simply denote $\bar{u}(a | c_\ast, c^\ast, G)$ as $\bar{u}(a | c_\ast, c^\ast)$
	\end{defi}
	\begin{teo}
	For any bounded decision problem with extreme consequences $c^\ast \succ c_\ast$ and any $G \succ \emptyset$
	\[ a_1 \succeq_G a_2 \Leftrightarrow \bar{u}(a_1) \geq \bar{u}(a_2). \]
	\end{teo}
	\begin{proof}
	See \cite{bernardo2000bayesian} Chapter 2 Proposition 2.22
	\end{proof}
	This result means that for any decision maker whose preferences satisfies axioms 1 through 5 the only election criteria that is compatible with the axioms is the maximization of expected utility. Of course, this result establishes a complete ordering of the options, but it does not guarantees the existence of an option for which the expected utility is maximum (\cite{bernardo2000bayesian}).
	\subsection{Game Theory}






\section{Problem Statement}
	\subsection{Problem Statement}
	\subsection{Motivation}
	Decision making under uncertain conditions is a fundamental part of intelligent reasoning (\cite{lake2017building}). Intelligent agents often face situations where an action must be chosen in the presence of uncertain conditions; this means that an outcome will be observed according to some probability distribution given the action chosen by the agent.
	
	In many real-world applications, the agent doesn't know all of the parameters required to calculate the maximum utility, but if the agent knew that his actions and the possible consequences were \textit{causally related}, then he could attempt to discover this relations and use them in order to predict consequences of actions better than if he only observes multiple action-outcome pairs as done in Reinforcement Learning (\cite{sutton1998reinforcement}). 
	
	It is known that human beings conceive their actions on the world as \textit{intervening} in the world. Following this idea, \cite{lattimoreNIPS2016} consider decision problems where the action to be chosen is an intervention over a known causal graphical model. The agent must choose the intervention that maximizes the value of a \textit{target variable} after a series of learning rounds. They model their problem as if choosing an intervention was choosing an arm of a \textit{slot machine}, in which a gambler chooses an arm and gets some reward. From the rewardsm they estimate probabilities and output an optimal action in the sense of obtaining minimal regret.
	
	 Their work considers that the causal model is fully known. They mention that the case where the causal model is unknown is left as an open question, and it is precisely what we are proposing to answer. 
	\subsection{Justification}
	Besides obtaining a criteria of choosing actions, our agent will also learn a causal model of his environment. Learning causal models is importante since it allows further questions to be asked. 
	\subsection{Research Question}
	\subsection{Hypothesis}
	Let $\mathcal{G}$ a causal graphical model whose probability distribution $P_\mathcal{G}$ satisfies (the axioms). Let $(D,E,c)$ a decision problem whose uncertain events are related to the variables in $\mathcal{G}$. Then, if a decision maker doesn't know the probabilities of the uncertain events in $E$,  by repeatedly making decisions he can learn causal information and use it to find the optimal actions (in the sense of expected utility theory) in less rounds than if he doesn't consider causal information.
	\subsection{Objectives}
		\subsubsection{General Objectives}
		\subsubsection{Specific Objectives}
		\subsubsection{Limitations}

\section{Related Work}

\section{Methodology}
\subsection{Modelling}
\subsection{Working plan}

\newpage
\bibliographystyle{apalike}
\bibliography{/Users/MauricioGS1/INAOE/Propuesta/Bibliografia.bib}
\end{document}
