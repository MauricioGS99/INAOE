\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
%\usepackage[spanish, mexico]{babel}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{listings}
\usepackage{breakcites}
\usepackage{dsfont}
\usepackage{hyperref}
\usepackage{amssymb,amsthm,amsmath,latexsym}
\usepackage[margin=1.5cm]{geometry}
\usepackage{natbib}
%\bibliographystyle{stylename}
%\usepackage{fancyhdr}
%\pagestyle{fancy}
\theoremstyle{plain}
\newtheorem{teo}{Theorem}
\newtheorem{prop}[teo]{Proposition}
\newtheorem{defi}[teo]{Definition}
\newtheorem{obs}[teo]{Observation}
\newtheorem{lem}[teo]{Lemma}
\newtheorem{cor}[teo]{Corolary}
\usepackage[pdftex]{color,graphicx}
\usepackage{tikz}
\usetikzlibrary{trees}
\usetikzlibrary{calc}
%\newcommand{\sgn}{\mathop{\mathrm{sgn}}}
\title{Use and acquisition of causal relations for decision making under uncertainty using imperfect information repeated games}
\author{Mauricio Gonzalez Soto}
\begin{document}
%\nocite{*}
\maketitle
\tableofcontents
\newpage
\begin{abstract}
We consider decision problems under uncertainty where the options available to a decision maker and the resulting outcome are related through a causal mechanism which is unknown to the decision maker, although he is aware of the causal nature of his environment. We study how a decision maker can learn about this causal mechanism through sequential decision making as well as using current causal knowledge inside each round in order to make better choices had he not considered causal knowledge. We propose a decision making procedure in which an agent holds \textit{beliefs} about her environment which are used to make a choice and then are updated using the observed outcome. As proof of concept, we present an implementation of this causal decision making model and apply it to a simple problem. We show that the model achieves a performance similar to the classic Q-learning while it also acquires a causal model of the environment. 
\end{abstract}
\section{Introduction}
\indent A fundamental part of intelligent reasoning is being able to make decisions under uncertain conditions (\cite{danks2014unifying}, \cite{lake2017building}, \cite{pearlwhy}). In some cases, a decision maker who faces an uncertain environment has enough information to make choices by maximizing expected utility, which is the classic formal criteria for making decisions if rational preferences are assumed (\cite{bernardo2000bayesian}, \cite{gilboa2009decision}). On the other hand, if enough information is not available, the decision maker could attempt to \textit{learn} from the environment by interacting with it.

Learning by interaction has been extensively studied by computer scientists using the Reinforcement Learning (RL) setting \cite{sutton1998reinforcement}, but the most common used techniques  in this field are purely associative and do not consider any high-level structure of the environment beyond what is expressable in a Markov Decision Process \cite{garnelo2016towards}.

A particular case of a \textit{higher level structure}; i.e., beyond associative patterns, is the case of \textit{causal} structure. A causal structure encondes a series of \textit{cause-effect} relations between events.

Since human beings are known to learn causal models in sequential decision making processes (\cite{sloman2006causal}, \cite{nichols2007decision}, \cite{meder2010observing}, \cite{hagmayer2013repeated}, \cite{danks2014unifying}), and even though this learning is not perfect \cite{rottman2014reasoning}, we propose that an autonomous agent can learn and use causal information while interacting with an uncertain environment which is governed by a fixed \textit{causal mechanism} which is unknown to the agent.  

The proposed way for an agent to learn from repeated interactions is by giving her \textit{beliefs} about the structure of the environment and a way to update them after an outcome has been observed.

While the standard setting in RL is to model the agent-environment interaction as an agent that moves from one \textit{state} to another inside a model of the environment and observing a reward as these transitions occur, we propose to model it as a \textit{game} between the decision maker and a player called \textit{Nature} which will select his actions from the causal model in response to what the decision maker has chosen. 

The agent, using her current beliefs, will generate a \textit{local} causal model and choose an action from it as if that model was the true one. Then, after she observes the consequences of her actions, her beliefs will be updated according to the observed information in order to make a better choice the next time. The agent, besides learning a policy to choose actions will also learn a causal model from the environment since the causal model she forms will approximate the true model.

Learning a causal model of the environment allows to extract high-level insights of a phenomena beyond associative descriptions of what is observed. A causal model is able to \textit{explain} why a particular decision was made since it allows to extract the causes and effects of an agent's actions.

\newpage
\section{Main Concepts}
	\subsection{Attempts to define Causality}
	Causality has been a difficult concept to define and several attempts have been made. In fact, in mainstream statistics  the term \textit{cause} have been nearly banned as documented by \cite{pearl2018why}.
	
	In the reference works on Causality (\cite{spirtes2000causation}, \cite{pearl2009causality}) the term \textit{causality} is defined in terms of common language and then the mathematical machinery required to model causality is provided. In this way, causality becomes defined as what is modeled by causal models, which are themselves defined as to model causality. This apparent circularity is deeply studied by \cite{woodward2005making}. For this reason, we will give here a formal working definition of causal relations which allows to be used formally.
	
	Several definitions of Causality exist in diverse contexts with diverse applications in mind, such as the Topological Causality for Dynamical Systems (\cite{harnack2017topological}), Lamport's Causality (\cite{lamport1978time}), Granger's Causality for Time Series (\cite{granger1969investigating}), Suppes' Causality (\cite{suppes1970probabilistic}) each with its own mathematical framework. A review of several of this theories, including Suppes' and Granger's can be found in \cite{holland1985statistics}.
	\subsection{Definition of Causality}
	Causality is understood as a \textit{stochastic} relation between \textit{events}. Some event \textit{causes} another event to occurr. For example, The formal definition of Causality that will be adopted in this work is the definition provided in \cite{spirtes2000causation}.
	\begin{defi}{\label{causal_relation}}
	Let $(\Omega, \mathcal{F}, \mathbb{P})$ a finite probability space, and consider a binary relation $\to \subseteq \mathcal{F} \times \mathcal{F}$ which is:
	\begin{itemize}
	\item Transitivity: If $A \to B$ and $B \to C$ for any $A, B, C \in \mathcal{F}$ then $A \to C$.
	\item Irreflexive: For all $A \in \mathcal{F}$ it doesn't hold that $A \to A$.
	\item Antisymmetric: For $A,B \in \mathcal{F}$ such that $A \neq B$ if $A \to B$ then it doesn't hold that $B \to A$.
	\end{itemize}
	We will say that $A$ is \textit{a cause} of $B$ (or that $A$ causes $B$, $A$ is the cause and $B$ is the effect) if $A \to B$.
	\end{defi}
	We will assume that for any event $A \in \mathcal{F}$ there are no causes lying outside $\mathcal{F}$, and that the relations expressed by $\to$ are the only causal relations in the environment. This assumption can be interpreted as not allowing \textit{intermediate} events between events known to be causally related. For example, if in our space we have that striking a match causes fire, we will not allow into consideration the underlying chemical reactions that cause fire from friction. In this sense, we will say that striking a match is a \textit{direct cause} of fire and this will be the only causes considered (\cite{spirtes2000causation}). 
	\subsection{Representation into a Directed Acyclic Graph}
	Making some notation abuse, the causal relations contained in $\to$ can be summarized in a graph $G=(V,E)$ in the following way: If $A \to B$ then the graph must contain a node $A \in V$ representing $A$, a node  $B \in V$ representing $B$ and a directed edge $e \in E$ connecting the respective nodes in the direction of the causal relation.
	\begin{prop}
	Given a causal relation $\to$ as in Definition \ref{causal_relation} then the graph that is obtained by considering nodes for events and edges for the causal relations as previously described is a Directed Acyclic Graph.
	\end{prop}
	\begin{proof}
	The graph is directed since the definition of causality imposes a direction between events; namely, a direction between the cause and the effect. To see that the graph is acyclic, suppose that a cycle $A \to B \to C \to A$ exists, this would imply, because of transitivity, that $A \to A$, which can not be since the relation is irreflexive.
	\end{proof}
	Notice that since the graph is finite, there exist some nodes that does not have causes, which are called \textit{exogenous}. If an $A$ event is caused by some other event, then we say it is \textit{endogenous} and we denote the set of its causes as $Pa(A)$.
	\subsubsection{Relation between the graph and probabilities}
	Given a Directed Acyclic Graph it is possible to obtain a probability measure that expresses the conditional independence relations that are expressed in the graph (\cite{koller2009probabilistic}), which we will call $P_\mathcal{G}$. For the DAG built from the causal relations, we require that its correspondent $P_\mathcal{G}$ satisfies the following conditions (\cite{spirtes2000causation}):
	\begin{itemize}
	\item Markov Causality: an event $V$ (or node in $\mathcal{G}$) is independent of every other event $A$ such that $A$ isn't either a cause nor an effect of $V$ given the causes of $V$.
	\item Causal Minimality: No proper sub-graph of $\mathcal{G}$ satisfies the Markov Causality condition.
	\item Causal Faithfulness: The Markov Causal condition contains all of the conditional independence statements expressed by the DAG $\mathcal{G}$.
	\end{itemize}
	This conditions are required in an axiomatic fashion so they will be taken as they are without questioning.
	\subsection{Causal Graphical Models}
	A causal graphical model (CGM) consists of a set of random variables $\mathcal{X}=\{ X_1,...,X_n \}$, an acyclic directed graph (DAG) whose nodes are in correspondence with the variables in $\mathcal{X}$ and whose edges represent relations of cause-effect. Also, the model, which up to this point is nothing more than a Bayesian Network with causal semantics, is enriched with an operator named $do()$ which is defined over graphs and whose action is described as follows: given $\mathbf{X} \subseteq \mathcal{X}$ and $\mathbf{x} = \{ x_{i_1}, x_{i_2}, ... , x_{i_j} \} \in Val(\mathcal{X})$ the action $do(\mathbf{X} = \mathbf{x} )$ corresponds to assign to each $X_j \in \mathbf{X}$ the value $x_{x_{i_j}}$ and to delet every incoming edges into the node corresponding to each $X_j$ in the graph $\mathcal{G}$ (\cite{koller2009probabilistic}, \cite{sucar2015probabilistic}) To apply the $do()$ operator over a variable (or set of variables) is also called as an \textit{intervention} over the variable.
	
	It is required that the probability distribution that results from an intervention over a variable is Markov compatible with the graph; this is, the resulting interventional distribution is equivalent to the product of the conditional probability of every variable given its parents in the intervened graph (\cite{sucar2015probabilistic}).
	\subsubsection{The identifiability problem}
	Under what conditions can causal inquiries be answered in terms of purely \textit{observational} data? It is known that if the Markov Causal condition and Causal Faithfulness hold, then it is possible to identify the causal graph up to Markov equivalence (\cite{peters2012identifiability}, \cite{mooij2016distinguishing}).
	\subsubsection{Do-Calculus}
	The Do-Calculus (\cite{pearl1995causal}, \cite{pearl2009causality}) is a set of rules for manipulating probabilistic statements that involve interventions and, under certain conditions, allow them to be transformed into statements that do not involve interventional data. 
	
	Some notation is important to be mentioned: Consider a causal graphical model $\mathcal{G}$ and $X,Y,Z$ disjoint sets of nodes of $\mathcal{G}$. We denote by $\mathcal{G}_{\bar{X}}$ the graph that is obtained by deleting from $\mathcal{G}$ all of the edges that enter nodes in $X$. In the same way, $\mathcal{G}_{\underline{X}}$ is the graph obtained by deleting the edges that emerge from $X$. Finally, $\mathcal{G}_{\underline{Z}\bar{X}}$ is the graph obtained by deleting edges incoming into $X$ and outgoing from $Z$.
	
	\begin{teo}{\label{docalculus}} (\cite{pearl2009causality})\\
Let $\mathcal{G}$ a CGM y $P_{\mathcal{G}}$ the probability measure induced by the model; then, for disjoint sets of nodes $X,Y,Z,W$ it holds:
\begin{itemize}
\item If for the graph $\mathcal{G}_{\bar{X}}$ it holds that $Y$ is conditionally independent from  $Z$ given $X$ y $W$, then
\[ P_{\mathcal{G}}(Y=y | do(X=x), Z=z, W=w) = P_{\mathcal{G}}(Y=y | do(X=x), W=w). \]
\item If for the graph $\mathcal{G}_{\bar{X}\underline{Z}}$ it holds that $Y$ is conditionally independent from $Z$ given $X$ y $W$, then
\[ P(Y=y | do(X=x), do(Z=z), W=w) = P(Y=y | do(X=x), Z = z, W=w). \]
\item Let $Z(W)$ the set of nodes in  $Z$ that aren't ancestors of any node in  $W$ in the graph $\mathcal{G}_{\bar{X}}$.   If $Y$ is conditionally independent from $Z$ given $X$ y $W$ in the graph $G_{\bar{X}, \bar{Z(W)}}$, then
\[ P(Y=y | do(X=x), do(Z=z), W=w) = P(Y=y | do(X=x), W=w). \]
\end{itemize}
\end{teo}
\begin{teo}{\cite{peters2017elements}}\\
The following statements hold:
\begin{itemize}
\item The Do-Calculus is complete; this is, sufficient for deriving every identifiable interventional distribution (\cite{huang2006pearl}, \cite{shpitser2006identification}).
\item There exists an algorithm capable of finding all of the identifiable interventions (\cite{tian2002}, \cite{huang2006pearl}).
\item A necessary and sufficient criteria exists for the identifiability of interventional distributions (\cite{shpitser2006identification}, \cite{huang2006pearl}).
\end{itemize}
\end{teo}
Las reglas del cálculo do proveen una solución al problema de la identificabilidad:
\begin{cor} (\cite{pearl2009causality}).\\
A distribution $q=P(y_1,...,y_k | do(x_1),...,do(x_n))$ is identifiable in a causal graphical model $\mathcal{G}$ if there exists a finite sequence of transformations, where each one of them corresponds to any of the rules of the Do-Calculus, that reduce $q$ to a purely observational expression.
\end{cor}
	
	\subsection{Decision Theory}
	A Decision Problem under Uncertainty is a situation in which an agent must choose one out of many actions with uncertain consequences. The most common theories for Decision Making under Uncertainty are those from \cite{von1944theory} and \cite{savage1954the}. In the former theory it is assumed that the decision maker knows the stochastic relation between actions and consequences, and in that case the theory guarantees that the decision maker behaves as if he maximizes the expected value of an utility function. On the other hand, if the decision maker doesn't know the probabilities of observing outcomes given a chosen action, then Savage's theory guarantees that the decision maker behaves as if he had in mind a \textit{subjective} probability distribution, a utility function and chooses the action which maximizes the expected utility with respect to that subjective probability distribution and utility function. 
	
Both von Neumann and Savage's Decision Under Uncertainty theories are known as the \textit{classic} theories in other areas, mostly in Economics. It is important to say that other Decision Making theories exist, such as Prospect Theory (\cite{kahneman1979prospect}), Case-Based Decision Theory (\cite{gilboa1995case}) among others that are out of the scope of this work.

For what remains of this section, we follow \cite{bernardo2000bayesian} in his exposition of Savage's theory since it is the most suitable for our purposes because we will be considering decision makers that do not know all of their environment. Savage's Decision Making theory, together with von Neumann's theory is what it is commonly known as \textit{classical} decision making. 
	
	\subsubsection{Elements of a Decision Problem}
	A Decision Problem under Uncertainty is composed by:
	\begin{itemize}
	\item A set $\Omega$ of \textit{states}.
	\item A set $\mathcal{A}=\{a_i : i \in I \}$ of actions.
	\item For each action $a_i$, a partition $E_i = \{ E_j : j \in J \}$ of $\Omega$. Define $\mathcal{E} = \cup_{i \in I } E_i$.
	\item For each action $a_i$, a set of consequences $C_i = \{ c_j : j \in J \}$. Define $\mathcal{C}=\cup_{i \in I} C_i$
	\item A preference relation $\succeq$ defined over $\mathcal{A}$ which represent the decision-maker whishes. It is assumed that the decision maker will choose among his options according to his preferences, which are represented by $\succeq$.
	\end{itemize}
	Since we are considering that a decision maker chooses an action $a \in \mathcal{A}$, and then an uncertain event $E$ will occur which will then produce the consequence $c$, we will identify actions in $\mathcal{A}$ as $a_i = \{ c_j | E_j : j \in J \}$. As a technical assumption required for all of the math to work, we require that elements of $\mathcal{C}$ belong to $\mathcal{A}$. This is achieved by adding into $A$ elements of the form $\{ c | \Omega \}$ for $c \in \mathcal{C}$. This being said, we can extend the preference relation $\succeq$ to elements of $\mathcal{C}$ and therefore indistinctively write $c_1 \succeq c_2$ for $c_1, c_2 \in \mathcal{C}$ or $a_1 \succeq a_2$ for $a_1,a_2 \in \mathcal{A}$. We require that $\mathcal{E}$ is an \textit{algebra}.	
	\begin{defi}
	From the prerefence relation $\succeq$ we can derive some other relations between actions:
	\begin{itemize}
	\item $a_1 \sim a_2 \Leftrightarrow a_1 \succeq a_2 \textrm{ and } a_2 \succeq a_1$
	\item $a_1 \succ a_2 \Leftrightarrow a_1 \succeq a_2 \textrm{ and it does not hold that } a_2 \succeq a_1$.
	\end{itemize}
	\end{defi}
	\begin{defi}
	A relation between events can be derived from $\succeq$ in the following way:
	\[E \succeq F \Leftrightarrow \textrm{ for all } c_1 \succeq c_2 \textrm{ it holds that } \{ c_2 | E, c_1 | E^c \} \succeq \{ c_2 | F, c_1 | F^c \}. \]
	In this case, we say that $F$ is \textbf{not more likely} than $E$. It can easily be shown that $\Omega \succ \emptyset$, and a relation $\sim$ for events is defined in an analog way than for consequences.
	\end{defi}
	\begin{defi}
	Given some $G \succ \emptyset$ we define a conditional preference relation $\succeq_G$ as follows:
	\begin{itemize}
	\item i) $a_1 \succeq_G a_2 \Leftrightarrow \textrm{ for all a } \{ a_1 | G, a | G^c \} \succeq \{ a_2 | G, a | G^c \}$.
	\item ii) $E \succeq_G F \Leftrightarrow \textrm{ for } c_1 \succeq_G c_2 \{ c_2 | E , c_1 | E^c \} \succeq_G \{c_2 | F , c_1  | F^c \}$ 
	\end{itemize}
	\end{defi}
	\begin{defi}
	Two events $E,F \in \mathcal{E}$ are said to be independent if and only if:
	\begin{itemize}
	\item 	
	\end{itemize}
	\end{defi}
	\subsubsection{Axioms of Coherence}
	We require the following axioms for the preference relation $\succeq$ which we will call Axioms of Coherence or Rationality, and define a Rational decision maker as a decision maker who will choose according to a preference relation and whose preferences satisfy the axioms (\cite{bernardo2000bayesian}, \cite{gilboa2009decision}).\\
	\\
	\indent \textbf{Axiom 1}.
	\begin{itemize}
	\item i) There exists consequences $c_1$, $c_2$ such that $c_1 \succ c_2$.
	\item ii) For all consequences $c_1, c_2$ and events $E,F \in \mathcal{E}$ either $\{  c_2 | E , c_1 | E^c\} \succeq \{ c_2 | F, c_1 | F^c \}$ or $\{ c_2 | F, c_1 | F^c \} \succeq \{  c_2 | E , c_1 | E^c\}$
	\end{itemize}
	
	\textbf{Axiom 2}.
	\begin{itemize}
	\item i) $a \succeq a$ for all $a \in \mathcal{A}$.
	\item ii) If $a \succeq b$ and $b \succeq c$ for $a,b,c \in \mathcal{A}$, then $a \succeq c$.
	\end{itemize}
	
	\textbf{Axiom 3}
	\begin{itemize}
	\item i) If $a_1 \succeq a_2$ then for all $G \succ \emptyset$ $a_1 \succeq_G a_2$.
	\end{itemize}
	
	\textbf{Axiom 4.}\\
	There exists a sub-algebra $\mathcal{S}$ of $\mathcal{E}$ and a function $\mu : \mathcal{S} \to [0,1]$ such that:
	\begin{itemize}
	\item i) $S_1 \succeq S_2$ if and only if $\mu(S_1) \succeq \mu(S_2)$.
	\item ii) If $S_1 \cap S_2 = \empty$ then $\mu(S_1 \cup S_2) = \mu(S_1) + \mu(S_2)$.
	\item iii) For any number $\alpha \in [0,1]$ and independent events $E,F$, there is an event $S$, which is called a standard event, such that $\mu(S)=\alpha$ and $S$ is independent from $E$ and from $F$.
	\item iv) If $S_1$ is independent from $S_2$ then $\mu(S_1 \cap S_2)=\mu(S_1)\mu(S_2)$.
	\item v) If $E$ independent of $S$, $S$ independent from $F$ and $E$ independent from $F$ then $E \sim S$ implies that $E \sim_F S$.
	\end{itemize}
	
	\textbf{Axiom 5.}
	\begin{itemize}
	\item i) If $c_1 \succeq c \succeq c_2$ then there exists a standard event $S$ such that $c \sim \{ c_2 | S, c_1 | S^c \}$.
	\item ii) For each event $E$ there exists a standard event $S$ such that $E \sim S$.
	\end{itemize}
	
	For interpretations and critiques of the axioms we refer the reader to \cite{bernardo2000bayesian}, \cite{binmore2008rational}, \cite{gilboa2009decision}, \cite{wakker2010prospect}, \cite{peterson2017introduction}.
	\subsubsection{Subjective Probability and Utility}
	\begin{defi}
	Given a preference relation $\succeq$, we define the (subjective) \textbf{probability} of an event $E \in \mathcal{E}$ as the real number $\mu(S)$ associated with the standard event $S$ such that $E \sim S$.
	\end{defi}
	The subjective probability thus defined satisfies all of the Kolmogorov's axioms of probability (\cite{bernardo2000bayesian}). So, we can use all of the mathematical machinery for classic probability measures, including the definition of conditional probability, Bayes' theorem, etc.
	
	As a technical requirement we need to consider two extra options, one that is preferred over every other consequence, which will be denoted as $c^\ast$ and other consequence $c_\ast$ which  the agent prefers every other consequence before $c_\ast$. This consequences need not have an interpretation, but they can be thought of as \textit{heaven} and \textit{hell} respectively for obvious (at lest in the Western hemisphery) reasons. Critiques of this extra technical consequences can be found in \cite{binmore2008rational}, \cite{peterson2017introduction}.
	\begin{defi}
	Consequences $c^\ast$ and $c_\ast$ are called respectively best and worst (extreme consequences) if for any other consequence $c \in \mathcal{C}$ then $c^\ast \succeq c \succeq c_\ast$. Decision problems in which we add extreme consequences are called bounded decision problems.
	\end{defi}
	\begin{defi}
	Given a preference relation $\succeq$ in a bounded decision problem we define the canonical \textbf{utility} $u(c)=u(c | c_\ast, c^\ast)$ of a consequence $c \in \mathcal{C}$ relative to the extreme consequences $c_\ast$, $c^\ast$, as the real number $\mu(S)$ associated with any standard event $S$ such that $c \sim \{c^\ast | S, c_\ast | S^c\}$.
	\end{defi}
	\subsubsection{Decision Criteria for Decision Problems}
	\begin{prop}
	For any bounded decision problem with extreme consequences $c_\ast$ and $c^\ast$ it holds:
	\begin{itemize}
	\item i) For all $c \in \mathcal{C}$, $u(c | c_\ast, c^\ast)$ exists and is unique.
	\item ii) The value of $u(c | c_\ast, c^\ast)$ is unafected by the occurrence of any event $G \succ \emptyset$.
	\item iii) $0 = u(c_\ast | c_\ast, c^\ast) \leq u(c | c_\ast, c^\ast) \leq u(c^\ast | c_\ast, c^\ast) = 1$.
	\end{itemize}
	\end{prop}
	\begin{defi}
	For a bounded decision problem and any event $G \succ \emptyset$ and $a = \{ c_j | E_j : j \in J \}$ we define the conditional expected utility of $a$ as
	\[ \bar{u}(a | c_\ast, c^\ast, G) = \sum_{j \in J} u(c_j | c_\ast, c^\ast)P(E_j | G). \]
	If $G = \Omega$ we simply denote $\bar{u}(a | c_\ast, c^\ast, G)$ as $\bar{u}(a | c_\ast, c^\ast)$
	\end{defi}
	
	\begin{teo}
	For any bounded decision problem with extreme consequences $c^\ast \succ c_\ast$ and any $G \succ \emptyset$
	\[ a_1 \succeq_G a_2 \Leftrightarrow \bar{u}(a_1) \geq \bar{u}(a_2). \]
	\end{teo}
	\begin{proof}
	See \cite{bernardo2000bayesian} Chapter 2 “Foundations”, Proposition 2.22.
	\end{proof}
	This result means that for any decision maker whose preferences satisfies axioms 1 through 5 the only election criteria that is compatible with the axioms is the maximization of expected utility and thus establishes a \textit{normative} criteria for decision making for rational agents. Of course, this result establishes a complete ordering of the options, but it does not guarantees the existence of an option for which the expected utility is maximum and further mathematical assumptions are required over the utility function $u$ in order to guarantee the existence of the maxmimum utility option (\cite{bernardo2000bayesian}).
	
	In the particular case where the algebra $\mathcal{E}$ contains only the set $\Omega$ as its only element we say that it is a decision problem without uncertainty and in this case applying the previous Theorem we obtain the decision criteria which consist in maximizing the utility function.
	
	The result extends previous work of \cite{von1944theory} who assumed that the decision maker knows the probabilties of the uncertain events and guarantees the existence of an utility function such that the decision maker chooses by calculating expected value of this utility function.
	\subsection{Game Theory}
	A \textit{game} arises when two or more \textit{rational} decision makers, or players, have to make decisions in a situation in which the outcome for each player is partly determined by the choices of other players (\cite{binmore2008rational}).
	
	Game Theory is an area of Mathematics which is used to model games. The basic assumptions of the theory is that each decision maker is rational, and they take into account their knowledge or expectations of other decision makers behavior.
	
	In this section we will review the basic models of Game Theory that are required to this thesis proposal, which are the normal-form game, and the extensive-form game. 
	\subsubsection{Building blocks}{\label{basic_models}}
	The normal-form game models a situation in which two or more agents interact and where each one of them will choose an action simultaneously. Also, it is assumed that all of the relevant aspects of the game are known for each player.
	
	We follow the exposition from \cite{osborne1994course}, \cite{binmore2007playing}, \cite{shoham2008multiagent}.
	
	\begin{defi}
	A normal-form game is a tuple $(N,A,(\succeq_i)_{i \in N})$ where:
	\begin{itemize}
	\item $N$ is a finite set of players.
	\item $A=A_1 \times ... \times A_n$ where each $A_i$ is the set of available actions for player $i$. Each $a=(a_1,...,a_n) \in A$ is called an action profile.
	\item For each player $i \in N$, a preference relation $\succeq_i$ defined over $A$.
	\end{itemize}
	\end{defi}

Because of the decision-making criteria shown in the last section we know that each player can replace his preferences $\succeq_i$ for a utility function $u_i$.

The prisoner's dilemma is the most common example of a normal-form game, where two suspects must either confess or remain silent about a crime and they can't communicate with each other. In this game, both of the players know the consequences of each \textit{outcome} and the rewards (utilities) associated with each outcome.

In some cases, the players do not know all of the relevant aspects of a game, such as the payoffs or the available actions to other players. It is this situation that is modeled by normal-form Bayesian games (\cite{osborne1994course}, \cite{shoham2008multiagent}).

\begin{defi}
A normal-form Bayesian game consists of:
\begin{itemize}
\item A set of states $\mathcal{S}$.
\item A finite set of players $N$.
\item For each player: a set of actions $A_i$ and as in the previous definition we define $A= A_1 \times \cdots \times A_n$.
\item For each player: a finite set $T_i$ of \textit{signals} that are observable to player $i$.
\item For each player: a signal function $\tau_i : \mathcal{S} \to T_i$.
\item For each player: a probability measure $P_i$ defined over $\mathcal{S}$ such that $P(\tau^{-1}_i (t_i)) >0$ for $t_i \in T_i$.
\item For each player: a rational preference relation $\succeq_i$ defined over the set of probability measures defined over $A \times \mathcal{S}$.
\end{itemize}
\end{defi}

The interpretation of the elements of this model is as follows: the set $\mathcal{S}$ contains the descriptions of the relevant characteristics for all players, each player has \textit{a priori} beliefs over this characteristics, which are stated by the probability measure $P_i$. When a play is to be realized, the world is in some $\omega \in \Omega$ and each player observes his $\tau_i (\omega)$. If a player observes the signal $t_i \in T_i$ then he concludes that the real state of the world is in the set $\tau^{-1}_i (t_i)$. Player $i$ updates his beliefs over $\omega \in \Omega$ to $P_i(\omega) / P_i(\tau^{-1}_i (t_i))$ if $\omega \in \Omega$ and cero otherwise. Bayesian games are also called \textit{incomplete information} games in the literature which is different from the \textit{imperfect information} game which will be soon defined.

The normal-form game models situations where players select their actions simultaneously, or when it does not matter who makes an action first. More realistic and complex situations involve some notion of temporal structure, or order among the players' actions. The extensive-form game attempts to include this notion of order. The standard references for both of this models are \cite{osborne1994course}, \cite{shoham2008multiagent}, \cite{10.1007/978-94-010-0189-2_25}.
\begin{defi}
A perfect information extensive game consists of:
\begin{itemize}
\item A finite set $N$ of players.
\item A set $A$ of available action.
\item A set $H$ of sequences called histories that satisfies:
	\begin{itemize}
		\item The epty sequence belongs to $H$.
		\item If $(a_k)_{k=1}^K \in H$ ($K$ can be infinite) and $L < K$ then $(a_k)_{k=1}^L \in H$.
		\item If $(a_k)_{k=1}^\infty$ satisfies $(a_k)_{k=1}^L \in H$ for every positive integer $L$, then $(a_k)_{k=1}^\infty \in H$
	\end{itemize}
\item If a history $(a_k)_{k=1}^K \in H$ with $K$ finite and if it doesn't exist a $K+1$ such that$(a_k)_{k=1}^{K+1} \in H$ then $(a_k)_{k=1}^K$ is said to be a terminal history. The set of terminal histories is called $Z$
\item A function $\chi : H \to 2^A$ that assigns a set  $\chi(h)$ of possible actions to be taken after a history $h$ has occurred.
\item A function $\rho$ that assigns to every non terminal history $h$ a player $\rho(h)$ who will choose an action after history $h$ has occurred.
\item A function $\sigma : H \times A \to H \cup Z$ that maps history-action pairs into a new  history such that  if $\sigma(h_1 , a_1) = \sigma (h_2,a_2)$ then $h_1 = h_2$ y $a_1 = a_2$
\item For each player $i \in N$ a preference relation $\succeq_i$.
\end{itemize}
\end{defi}

In this model, each player knows how did he get up to certain point (knows what history occured). To allow more generality, we consider cases where each player doesn't know how he got up to certain point; i.e., he does not know which history happen and he can't distinguish in which node is he.
\subsubsection{Repeated Games}
\subsubsection{Learning in Games}

\newpage
\section{Problem Statement}
	\subsection{Problem Statement}
	Let  $\mathcal{G}$ a causal graphical model and let $(\mathcal{A},\mathcal{E},\mathcal{C})$ a decision problem under uncertainty whose actions $a_i = \{ c_j | E_j : j \in J \}$  are causally related to consequences $c \in \mathcal{C}$ through the uncertain events $E \in \mathcal{E}$ which correspond to variables in $\mathcal{G}$. And consider a rational decision maker who doesn't know the parameters of the causal model which control the probabilities of observing a consequence given an action $a \in \mathcal{A}$, we ask how the decision maker could learn about the causal structure that controls his environment in order to make good choices with respect to the decision makers' preferences.
	\subsection{Motivation}
	Decision making under uncertain conditions is a fundamental part of intelligent reasoning (\cite{lake2017building}). Intelligent agents often face situations where an action must be chosen in the presence of uncertain conditions; this means that an outcome will be observed according to some probability distribution given the action chosen by the agent.
	
	In many real-world applications, the agent doesn't know all of the parameters required to calculate the maximum utility, but if the agent knew that his actions and the possible consequences were \textit{causally related}, then he could attempt to discover this relations and use them in order to predict consequences of actions better than if he only observes multiple action-outcome pairs as done in Reinforcement Learning (\cite{sutton1998reinforcement}). 
	
	It is known that human beings conceive their actions on the world as \textit{intervening} in the world (\cite{hagmayer2009decision}). Following this idea, \cite{lattimoreNIPS2016} consider decision problems where the action to be chosen is an intervention over a known causal graphical model. The agent must choose the intervention that maximizes the value of a \textit{target variable} after a series of learning rounds. They model their problem as if choosing an intervention was choosing an arm of a \textit{slot machine}, in which a gambler chooses an arm and gets some reward. From the rewards they estimate probabilities and output an optimal action in the sense of obtaining minimal regret. Their work considers that the causal model is fully known. They mention that the case where the causal model is unknown is left as an open question, and it is precisely what we are proposing to answer. 
	\subsection{Justification}
	Many real-world applications of decision making are solved by \textit{associative} methods which capture only statistical patterns that are found in data. For example, current methods in Reinforcement Learning, and specifically in Deep Reinforcement Learning, although they have good performance in the task that were supposed to solve, they can not explain \textit{why} a specific trajectory was chosen by the algorithm. This is highly relevant in real-world applications such as self-driving cars, where it is very important to understand why an accident happened. 
	
	For example, as told by \cite{bornstein2016artificial}, at the University of Pittsburgh Medical Center a team of researchers tried to use Machine Learning to predict whether pneumonia patients might develop severe complications. For this purpose they trained Neural Networks and Decision Trees using the hospital's own data. Neural Networks outperformed Decision Trees, but only by studying the decisions made by the latter did the doctors find out that the algorithms instructed doctors to send home pneumonia patients who already had asthma, despite the fact that asthma sufferers are known to be extremely vulnerable to complications. The problem relied in the training data, because the hospital policy was to automatically send asthma sufferers with pneumonia to intensive care, and this policy worked so well that asthma sufferers almost never developed severe complications. It was only through the interpretability of the Decision Trees that the doctors didn't send asthma patients with pneumonia home to a certain death. 
	
	Methods based in Deep Neural Networks aren't supposed to explain why a certain output was produced since those methods are based in \textit{parallel distributed representations} and the same goes for any other learning algorithm that uses Deep Neural Networks, like Deep Reinforcement learning; when AlphaGo (\cite{silver2017mastering}) defeated the world-champion, humanity didn't learn anything new about the game of Go, because the algorithm was not designed to explain its moves, it was just curve fitting.
	
	Learning a causal model of an environment and using it to act upon the environment allows to \textit{explain} aspects of the model that a purely associative model would not be abel to explain. It allows to ask \textit{why}. 
	
	The impact of Causal Inference goes beyond Machine Learning and Computer Science. For example, economists are interested in understanding what did certan public policy caused (\cite{athey2017impact}) or how human decision makers that show \textit{inconsistent} preferences over time could be oriented if the causal consequences of this inconsistent behavior is introduced into a decision-making model (\cite{albers2016motivating}). In complex adaptive systems, causal relations can be used to clarify the complex interactions between agents in the system (\cite{abbott2017complex}) as well as for prediction and planning (\cite{hunt2016ants}, \cite{brock2018causality}).
	
	
	\subsection{Research Question}
	How a rational decision maker who faces an uncertain environment which is governed by a causal mechanism can learn and make use of this causal structure in order to make good choices? How can causal structure help a decision maker in order to guide his learning process?
	\subsection{Hypothesis}
	Let $\mathcal{G}$ a causal graphical model and let $(\mathcal{A},\mathcal{E},\mathcal{C})$ a decision problem whose actions $a_i = \{ c_j | E_j : j \in J \}$  are causally related to consequences $c \in \mathcal{C}$ through the uncertain events $E \in \mathcal{E}$ which correspond to variables in $\mathcal{G}$. Then, if a decision maker doesn't know the probabilities of the uncertain events in $E$, by repeatedly making decisions he can learn causal information and use it to find the optimal actions (in the sense of expected utility theory) in less, or equal, rounds than if he doesn't consider causal information.
	\subsection{Objectives}
		\subsubsection{General Objectives}
			The proposed research has as a general objective to find how a rational autonomous agent who faces an uncertain environment can discover and use causal information in order to make good choices that point towards a concise objective such as the maximization of some function.
		\subsubsection{Specific Objectives}
		In order to achieve the General Objective, we need to separate it into smaller problems which are required to be solved first. 
		
		
		\subsubsection{Limitations}
		The proposed research has some limitations. We are considering a \textit{rational} decision maker whose preferences can be represented as if maximizing an expected utility with respect to a subjective probability distribution and an utility that assigns numbers to possible outcomes. Maximization of expected utility is not the only decision making criteria as other theories exist, such as Prospect Theory (\cite{kahneman1979prospect}) where a decision maker considers a reference point and from there he considers losses more painfull than gains. Other theory is Gilboa's Case-Based Decision Theory, where a decision maker considers how similar is the current decision problem with problems faced in the past. 
		
		We consider a particular definition of Causality, which is Spirtes' definition (\cite{spirtes2000causation}) who defines causality as a stochastic relation between events. We take this definition of causality as well as its theoretical requirements as axioms and we do not question if is the definition most suitable for a certain situation.
		
		We are not considering that the player called Nature has any intentions nor objectives since we are given her constant payments in every outcome. A scenario where Nature could have objectives to pursue if it is modelling a player such as a Government, who can only act according to current laws but has general social objectives, such as the maximization of people with jobs in a negotiation with Unions.
		
		\subsubsection{Extensions}
		Given the Limitations addressed in the previous section, some possible extensions could be:
		\begin{itemize}
		\item Consider longer games.
		\item Consider more players than an agent and Nature.
		\item Consider objectives/intentions for Nature.
		\item Studying game-theoretical properties of the model such as equilibria in terms of the causal structure.
		\end{itemize}

\newpage
\section{Related Work}
Since we are considering decision problems where an agent must choose some available action and then receive a reward from it and after a set of stages the agent is expected to know a good action. It has been shown that the probability of the action selected in the $n$-th round not being optimal has an upper bound (\cite{audibert2010best}) if the optimality criteria is choosen to be the \textit{minimal regret} which is basically the difference of the theoretical mean reward of the optimal action and the chosen action.

On the other hand, if the largest mean reward is desired with a fixed confidence of $\delta$ then an algorithm for finding the optimal action exist in such a way that the number of rounds to find it is within a constant factor of a lower bound (\cite{jamieson2014lil}). 

If one does not consider a fixed confidence but instead a fixed number of learning rounds (known as fixed budget), then we face a \textit{fixed budget} problem where a lower bound for the probability of finding the optimal arm is given by \cite{carpentier2016tight}.

Notice that none of the previous works consider any kind of causal structure. For this kind of problems, causal relations between actions and consequences could be considered, in this way an action could be conceived as an \textit{intervention} over some environment, which is in fact how humans consider actions in the world (\cite{hagmayer2009decision}). Consider a set of variables that are causally related between them and to a \textit{reward} variable whose value has to be maximized by intervening one of the variables. It is shown by \cite{lattimoreNIPS2016} that adding causal information in a fixed budget decision problem allows the decision maker to learn \textit{faster} had he not considered causal information. Their work requires that the causal model is fully known to the decision maker, this requirement is relaxed later by \cite{sen2017identifying} who requires only that some part of the causal model is known and allow interventions over the unknown part.

Discovering the causal model itself while using current knowledge to make choices is left as future work in both \cite{lattimoreNIPS2016} and \cite{sen2017identifying}, but algorithms to discover causal relations in data can be found in \cite{eberhardt2008almost}, \cite{hauser2012two}, \cite{hyttinen2013experiment}, \cite{loh2014high}, \cite{shanmugam2015learning} \cite{mooij2016distinguishing}. Also, an \textit{active learning} approach can be used to learn causal models from data, where one starts with some initial graph and then select the instances in the data that allow to add and orient edges in order to end with a fully oriented graph. Active learning algorithms for causal discovery can be found in \cite{tong2001active},\cite{murphy2001active}, \cite{meganck2006learning}, \cite{he2008active}, \cite{hauser2012two}, \cite{10.1007/978-3-319-56970-3_9}, \cite{rubenstein2017probabilistic}.

Notice that these papers consider learning while interacting with a causal environment. We propose to model this interaction as a game, which has been previously considered in \cite{werling2015job}, who consideres an \textit{on-line} classification problem and modeled it as a game and where an oracle is used to classify initial observations.

\newpage
\section{Methodology}
In this section we describe the steps that are to be followed in order to answer the Hypothesis.
\subsection{Modelling a Decision Problem under Uncertainty as a Game}
Consider a decision problem under uncertainty $(\mathcal{A},\mathcal{E},\mathcal{C},\succeq)$ where the available actions $\mathcal{A}$ to a decision maker are causally related to the outcomes $\mathcal{C}$ and this causal relations are captured by a Causal Graphical Model $\mathcal{G}$. The Causal model is assumed to remain fixed and unknown to the decision maker, who is assumed to be rational and aware of the causal nature of the decision problem he faces.  

As stated in the Hypothesis, we propose that the agent learns from the environment by interacting with it. By interaction we mean a number of rounds where the decision maker will act and then observe the outcome of his action. 

Since the observed outcomes, given an action, are guided by the Causal Model, we can think of the environment as a decision maker whose available actions depend on what has been chosen previously by the original decision maker. We have mentioned that Game Theory allows to model situations where two or more decision makers interact. In this way, the first step in order to solve the stated problem is to model the interaction of the (original) decision maker and his environment as a \textbf{game} between two players: the decision maker, and a player which will be called Nature, whose actions are to be guided by the causal model. Since in decision problems under uncertainty it is assumed that the \textit{state} of the environment is unknown, we will assume that player Nature has the first move and he assigns some state to any of the variable in the causal model $\mathcal{G}$.

After the decision maker makes his play, Nature will \textit{respond} to the action selected according to the causal relations expressed by $\mathcal{G}$. This action-response dynamic forces to consider some notion of order between the players' moves, and because of this reason we must use the \textit{extensive-form} games described in Section \ref{basic_models}.

The extensive-form game considers two possibilities: perfect or imperfect information, where in the latter a player may not know what actions have been played in the past by other players, which is what we need to use because the decision maker does not know what move has Nature selected. 
\subsubsection{Elements of the Game}
The game which will be used to model the interaction between a decision maker and his environment is composed by:
\begin{itemize}
\item \textbf{Players:} The decision maker and a player called Nature, who will move first in order to assign a state.
\item \textbf{Actions:} The actions available to the decision maker are the same ones as in the original decision problem. Nature must choose according to the causal model.
\item \textbf{Preferences:} The preference relation to the decision maker is his same rational preference relation as in the decision problem. Nature is indifferente between outcomes.
\end{itemize}

\subsection{Belief Formation}
Once a Game that models agent-environment interaction is defined, we must turn to the question about \textit{learning} about the environment, which includes acquiring causal information and also using current causal knowledge in order to obtain new information.

For the agent to learn and reason about the plays of Nature we assume that he is able to observe the final outcome of the game, which includes knowing about the inicial uncertain state that Nature assigned. The agent, at the beginning of the game is uncertain about the state of the environment which he is facing, but he still must take an action, so in order to allow the decision maker to use his knowledge to act, he will have \textit{beliefs} about any relevant aspects of the environment which he will use as if they were the true model in any given play. These beliefs will be \textit{updated} according to what is observed at the end of each game.

Beliefs represent ignorance about relevant characteristics of certain environment (\cite{bernardo2000bayesian}, \cite{peterson2017introduction}), and thus are encoded as probability distributions, which usually come from a parametric family. 

In our proposed research, beliefs will encode current knowledge about the \textit{causal structure} of the environment, and this beliefs will be updated when information about actions-outcomes becomes available, which will happen at the end of each game. 

Beliefs must also encode any previous knowledge possesed by the decision maker, and this initial assignment must be coherent with the actions prescribed with the rationality axioms for preferences. In \cite{billot2005probabilities} is shown how to build a set of beliefs given a set of past observations in an axiomatic way.

\subsection{Belief updating}


\subsection{Solving first simpler cases}
Once that the model has been established, we proceed to further examine the problem of acquiring and using causal information in a decision problem and we notice three important cases, in order of ascending difficulty. In all three cases we consider a decision maker who faces an uncertain environment and who must take an action in order to satisfy his preferences. The uncertain environment in which the decision maker (or agent) exist is governed by a causal graphical model $\mathcal{G}$ which relates actions taken by the decision maker with outcomes. We are assuming that one of the variables of the casusal model is of interest for the decision maker in the sense that he seeks to maximize its value. This variable will be known as \textit{target variable} or reward variable.

The three cases that we notice are:
\begin{itemize}
\item The decision maker knows the causal model.
\item The decision maker knows only the graphical \textit{structure} of the causal model.
\item The decision maker knows nothing about the causal model. 
\end{itemize}
In the following we show how the first two cases, which are particular cases of our general problem, can be solved and we show that the proposed solution for the first two cases sheds light on how the general case could be solved.
\subsubsection{Fully knowing the causal graphical model}
If the decision maker knows the causal graphical model, then he could easily calculate the effect on the target variable that any of his actions has and then choose the action that has the highest probability of causing a \textit{desired} value of the target variable. The decision thus made is the one that maximizes the expected utility, by construction. Also, choosing this action is the \textit{best response} action, and therefore a Nash equilibrium for the game. 
\subsubsection{Knowing only the graph structure}

\subsubsection{General case: Model unknown}

\subsection{Validation of the proposed methods}

\subsection{Experimental methodology}
\subsubsection{Internal Validation: Reproducibility}
\subsubsection{External Validation: Different problems}

\subsection{Working plan}
Concrete steps towards solving the problem:
\begin{itemize}
\item Literature review, understanding the problem, considering several models and possible solutions.
\item Formalize the model. 
\end{itemize}
\subsubsection{Publication plan}
\begin{itemize}
\item Gonzalez-Soto, M., Sucar, L.E., Escalante, H.J., \textbf{Playing against Nature: causal discovery for decision making under uncertainty}. Accepted for a poster presentation at the CausalML 2018 workshop at ICML / IJCAI / AAMAS 2018.
\end{itemize}

\newpage
\section{Preliminary Results}
To show the factibility of this proposal, we considered a test scenario and two cases in ascending difficulty.
\subsection{Test scenario}
Consider a sick patient who arrives at a hospital and he either has disease $A$ or disease $B$. The doctor can either give him some pill or send him into surgery.  Both treatments entail risks and whether the treatment cures the patient or not depends on which disease it had originally. The doctor could be facing a mutation from a known disease, so she has some knowledge about what could happen if a treatment is given to the patient. Using her previous knowledge as a true model, she can choose a treatment and observe the outcome from which she will learn about this disease, so she could make a better decision the next time a similar patient arrives.

The causal model that governs this situation is shown in Figure \ref{causal_model}. The parameters for this model were fixed intuitively in such a way that each treatment is effective for only one disease, but the most effective treatment is riskier.

The variables in the model are: 
\begin{itemize}
\item \textbf{Disease:} Either $A$ or $B$.
\item \textbf{Treatment:} Either pill or surgery.
\item \textbf{Reaction:} Either dying or surviving.
\item \textbf{Lives:} Either living or dying.
\end{itemize}

The variables are causally related as shown in Figure \ref{causal_model}.

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[scale=0.5]{/Users/MauricioGS1/INAOE/Segundo_Semestre/Seminario/Semana12/codigo/causal_graph.png}}
\caption{Causal graphical model for the test scenario: the target variable \textit{Lives} is causally influenced by the disease the patient has, the treatment assigned and the survival to the secondary effects of treatment.}
\label{causal_model}
\end{center}
\vskip -0.2in
\end{figure}

The variable \textit{Lives} is the \textit{target variable} and, in this example, the only variable that can be intervened upon is the variable \textit{Treatment}. The decision maker prefers an outcome in which the patient lives.

In this scenario, Nature's move will consist in randomly assigning a disease to the patient. Then, the medic will asign a treatment using his current beliefs about the disease and the possible outcomes. The decision nodes for this play of the medic form an \textit{information set} because the medic doesn't know how she arrived there since she doesn't know what disease did Nature assign. Finally, Nature will sample the consequence of the treatment from the causal model and the medic will observe the outcome.

For this test scenario whose causal graphical model is shown in Figure \ref{causal_model}, we see  by applying the Pearl's do-calculus that the interventional distribution $P_{do(Tr)}(Y)$ is given by
\[ P(Y | do(Tr))=P(Y | D, Tr, R)P(R | Tr) P(D). \]
\\
In fact, from the structure of the model, which is shown in Figure \ref{causal_model}, we see that the involved probabilities in any calculations are:
\[ P(\textrm{disease}), P(\textrm{treatment}), P(\textrm{reaction} | \textrm{treatment}), \]
and
\[P(\textrm{lives} | \textrm{disease, treatment, reaction}). \]
\\
We can also see that the joint distribution for all of the variables can be expressed as
\[ P(Y | D, Tr, R)P(R | Tr) P(D)P(Tr). \]
This expression will be useful when specifying beliefs about the model as probability distributions.

\subsection{Case 1: The causal model is completely known}
If the causal model is completely known to the decision maker, then in one step she can obtain the probability for her desired value of the target variable, which in this case is the value corresponding to the outcome in which the patient lives at the end. Using this probability, she can choose which treatment to assign. Since this action maximizes the probability of the occurence desired value, it maximizes the expected utility, and it is also a \textit{best response} to the player Nature.

\subsection{Case 2: Only the structure is known}
Since the decision maker knows the graph structure, he can explicitly find a non interventional expression for the interventional distribution and update his beliefs about these unknown quantities. If the decision maker were not allowed to know, at the end of each round, the play of the Nature then this will have to be estimated as a hidden variable using, for example, the EM algorithm \cite{dempster1977maximum}, but meanwhile we are assuming that this information is available at the end of each round.

Given the structure of the model; i.e., the variables in it and the directed edges, the joint distribution of those variables can be expressed as a product of the form $P(X_j | Pa(X_j))$ where $Pa(X_j)$ are the parents of $X_j$ in the underlying DAG in $\mathcal{G}$. Since these distributions fully characterize the model, the decision maker will have beliefs over each one of these parameters. Notice that each of these parameters is itself a distribution of length equal to the number of possible values of the variable which is being conditioned, call the maximum number of possible values $k$ . 

A distribution suitable to modelling discrete probability vectors is the $k$-dimensional Dirichlet distribution, whose support is the set of probability vectors of length $k$ \cite{hjort2010bayesian}. The $k$ dimensional Dirichlet distribution has a density $f$ with respect to the Lebesgue measure given by

\[ f(x_1,...,x_k | \alpha_i,...,\alpha_k)=\frac{1}{B(\alpha)}  \prod_{i=1}^k x_i^{\alpha_i-1},\]

where $(x_1,...,x_k)$ are such that $\sum_{i=1}^k x_i =1$ and $\alpha=(\alpha_1,...,\alpha_k)$. The Dirichlet distribution is useful since it is conjugate for itself \cite{bernardo2000bayesian}.

In this way, the decision maker will have beliefs about the CPT's in the form of parameters of several Dirichlet distributions. Using the agent's current beliefs, a causal graphical model can be specified. Using this fully specified (structure + parameters) as a true model, the decision maker will make her choice as in Case 1. When the decision maker observes the value of the target variable, she will update the parameters that specify her beliefs.

Previously we argued that the agent's beliefs were going to be \textit{distributions} over a suitable space, but what is going to be updated are the parameters of such distributions. Namely, the $\alpha$ corresponding to the Dirichlet random variable assigned to each CPT.

For the belief updating, given a new data point,  two cases must be considered:
\begin{itemize}
\item The variable to update has no parents.
\item The variable to update has parents.
\end{itemize}

In the first case, if a prior Dirichlet($\alpha$) is used, then the posterior is given by
\[ \textrm{Dirichlet}(\alpha + c) \]
where $c$ is a vector of the number of occurrences of that observed data point. 

For the second case, we must consider both the occurrences of that data point as well as the parents for each of the variables. Following \cite{barber2012bayesian} we denote as $\theta_i(X,j)$ the number of times the event $\{X=i | Pa(X)=j\}$ is observed. In this case, if the prior of $X_i$ conditioned on its parents having the value $j$ is given by a a Dirichlet($\alpha$), then the posterior for the variable $X_i$ given an observed data point is given by 
\[ \prod_j \textrm{Dirichlet}(\alpha + \theta_i(\cdot,j)). \]

\subsubsection{Implementation}
We begin with a random assignation of the $\alpha$ parameter for each of the distributions considered. We use Dirichlet distribution for each of the conditional probability tables that appear in the factorization of the joint probability for the graph of $\mathcal{G}$. Since each of the variables in the model is binary, then the product of these Dirichlets is Dirichlet.

With this parameters, the decision maker forms a causal model and chooses the action that maximizes the probability of the desired value for target variable as in Case 1.  With this action chosen, we simulate an outcome from the causal graphical model using the chosen action as an intervention. This evidence is used to update the parameters, which then will be used to generate a new causal model, and so on.

We show the results of two experiments. We compare the performance obtained by the causal agent, a \textit{random agent} who selects his actions at random, and an agent performing Q-learning. We show the average perform over $20, 50, 100$ and $200$ rounds.


\begin{center}
\begin{minipage}{0.48\linewidth}
\includegraphics[width=\linewidth]{/Users/MauricioGS1/INAOE/Segundo_Semestre/CausalML_Workshop/figures/20_rounds_format.png}
\captionof{figure}{Average reward in 20 rounds}
\end{minipage}%
\hfill
\begin{minipage}{0.49\linewidth}
\includegraphics[width=\linewidth]{/Users/MauricioGS1/INAOE/Segundo_Semestre/CausalML_Workshop/figures/50_rounds_format.png}
\captionof{figure}{Average reward in 50 rounds}
\end{minipage}
\end{center}
\begin{center}
\begin{minipage}{0.49\linewidth}
\includegraphics[width=\linewidth]{/Users/MauricioGS1/INAOE/Segundo_Semestre/CausalML_Workshop/figures/100_rounds_format.png}
\captionof{figure}{Average reward in 100 rounds}
\end{minipage}
\hfill
\begin{minipage}{0.49\linewidth}
\includegraphics[width=\linewidth]{/Users/MauricioGS1/INAOE/Segundo_Semestre/CausalML_Workshop/figures/200_rounds_format.png}
\captionof{figure}{Average reward in 200 rounds}
\end{minipage}
\end{center}

\subsection{Case 3 and future work: The model is not known}
The causal model were fully unknown, the decision maker will have to deal with the problem using only any previous knowledge and her own intuitions. Again, any previous knowledge and considerations will be expressed as \textit{beliefs} about the uncertainties in the environment, which will take the form of a probability distributions over a suitable space. 

As in the previous case, we consider a repeated game where the base game consists of Nature assigning a random state of the environment and responding to the agents choices with the effects that were caused by her decisions. In this game, as well as in the previous one, the decision maker will attempt to learn by updating, and using, beliefs in a suitable way. 

The most notable difference with the previous case is that the \textit{structure} of the model is also to be learned in such a way that both the structure and parameters converge to the true model in the limit. In the previous case the decision maker knew the form of the Conditional Probability Tables (CPT) involved in any calculation. In this case, she doesn't know the structure of the DAG so which CPT's are involved is unknown.

If the decision maker knew which variables appear in the true model that governs the environment, even though she didn't know how they are connected, she could use a \textit{Dirichlet Process} to generate Dirichlet distributions and generate causal graphical models the same way as in Case 2 and updating the parameters of the process using the observed information. The Dirichlet Process\footnote{with parameters $M,G_0$}, which was introduced by \cite{ferguson1973bayesian}, is random measure defined over a space $S$ such that for each partition $B_1,...,B_k$ the vector $(G(B_1),...,G(B_k))$ follows a Dirichlet distribution \cite{hjort2010bayesian}, \cite{muller2016bayesian}, \cite{ghosal2017fundamentals}. 

Belief updating using causal information when the decision maker doesn't know the structure of the model nor its parameters is yet to be studied and left as future work. 

%In Figure \ref{20_rounds} we observe the average rewards for each agent in 20 rounds of decision making. Here we notice that Q-learning outperforms our algorithm, which has a similar performance as the random choosin procedure until round 11.
%
%\begin{figure}[ht]
%\vskip 0.2in
%\begin{center}
%\centerline{\includegraphics[scale=0.5]{/Users/MauricioGS1/INAOE/Segundo_Semestre/CausalML_Workshop/figures/20_rounds_format.png}}
%\caption{Average reward obtained in each round for each agent}
%\label{20_rounds}
%\end{center}
%\vskip -0.2in
%\end{figure}
%
%In Figure \ref{50_rounds} we observe the average rewards for each agent in 50 rounds of decision making. Our algorithm follow closely the Q-learning agent and outperform the random agent.
%
%\begin{figure}[ht]
%\vskip 0.2in
%\begin{center}
%\centerline{\includegraphics[scale=0.5]{/Users/MauricioGS1/INAOE/Segundo_Semestre/CausalML_Workshop/figures/50_rounds_format.png}}
%\caption{Average reward obtained in each round for each agent}
%\label{50_rounds}
%\end{center}
%\vskip -0.2in
%\end{figure}
%
%In Figure \ref{100_rounds} we observe the average reward obtained by the three agents in 100 rounds, where our algorithm slightly outperforms Q-Learning.
%
%\begin{figure}[ht]
%\vskip 0.2in
%\begin{center}
%\centerline{\includegraphics[scale=0.5]{/Users/MauricioGS1/INAOE/Segundo_Semestre/CausalML_Workshop/figures/100_rounds_format.png}}
%\caption{Average reward obtained in each round for each agent}
%\label{100_rounds}
%\end{center}
%\vskip -0.2in
%\end{figure}
%
%In Figure \ref{200_rounds} we observe the average reward obtained by the three agents in 200 rounds. The average reward obtained is very similar for Q-learning and our algorithm.
%
%\begin{figure}[ht]
%\vskip 0.2in
%\begin{center}
%\centerline{\includegraphics[scale=0.5]{/Users/MauricioGS1/INAOE/Segundo_Semestre/CausalML_Workshop/figures/200_rounds_format.png}}
%\caption{Average reward obtained in each round for each agent}
%\label{200_rounds}
%\end{center}
%\vskip -0.2in
%\end{figure}

We see that our method obtains a very similar reward as the classic Q-learning algorithm for a larger number of rounds, where the random agent is outperformed, but our model offers something extra because it learns a causal model of the environment. 

\newpage
\bibliographystyle{apalike}
\bibliography{/Users/MauricioGS1/INAOE/Propuesta/Bibliografia.bib}
\end{document}
