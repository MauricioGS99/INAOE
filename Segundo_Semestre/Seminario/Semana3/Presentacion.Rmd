---
title: "Research Problem"
author: "Mauricio Gonzalez"
date: "5/2/2018"
output:
  ioslides_presentation: default
  beamer_presentation: default
  slidy_presentation: default
bibliography: /Users/MauricioGS1/INAOE/Segundo_Semestre/Propuesta/Bibliografia.bib
---

# Introduction
## Intuition
- While playing a videogame or learning how to drive, human beings are forced to *make decisions* and to *actively intervene* in the world.
- This active *decision-making* process modifies the state of the world, which allows us to learn about *causal relations* that hold in our environment
- We ask ourselves how an *intelligent agent* can acquire and *use causal knowledge* while engaging with its environment to achieve some task.

----

- The agent will interact with its environment and receive a feedback signal based on the actions taken by him, which will modify his cur- rent causal knowledge.
![action-feedback loop](/Users/MauricioGS1/INAOE/Primer_Semestre/Figures/cry.png)

----

- Before the next step is taken, the current causal knowledge will be taken into account when choosing the next action together with the current value of each posible action.
![action-feedback loop](/Users/MauricioGS1/INAOE/Primer_Semestre/Figures/baby.png)

----

- This new action and its outcome will again modify the causal knowl- edge, which will improve the next action and so on.
```{r fig.width=5, fig.height=5,echo=FALSE}
library(png)
library(grid)
img <- readPNG("/Users/MauricioGS1/INAOE/Primer_Semestre/Figures/Proposed.png")
grid.raster(img)
```

## Fields involved
- Learning by interaction.
- Sequential Decision Making.
- Causal learning.
- Bayesian inference.

# Research Problem
## Statement
- **Acquiring causal kwnoledge via interaction with the environment and using this knowledge to guide a decision-making process**
- Lay a bridge between Reinforcement Learning and Causal Learning: fundamental assumptions.
- Why reinforcement learning? Interventions are needed in order to acquire causal knowledge and RL provides an action-reaction interactive loop.

## Relevance
- Current RL methods are purely reactive.
- A lot of training time is required.
- Do not allow high-level reasoning.
- Learning causal structure will allow the agent to make better decisions because it will be able to consider consequences of his actions, and not only reacting to rewards.

## Feasibility
- We know how to learn by interaction (from Reinforcement Learning)
- We know that causal learning can be acquired in human beings through sequential decision making (from Computational Psychology).
- We know that *best action* can be selected from causal data (from sequential experimental design).

----

- Therefore, the elements exist and can be knitted together, although it will not be an easy task. 
- The elements may exist, and although core ideas are shared in the fine details are far away between them.
- The Devil is in the details.

## Disclosure
- Causality as in [@pearl2009causality] and [@spirtes2000causation]
- Other definitions of Causality exist.

# State of the Art
## Main groups and people
* Causality
    + Carnegie Mellon (Spirtes)
    + UC Los Angeles (Pearl)
    + MIT (Joshua Tenenbaum)
    + Max Planck Institute for Intelligent Systems (Schölkopf)
    + University of Copenhagen  (Jonas Peters)
    + Edinburgh (Albrecht, Subramanian Ramamoorthy)
    
----

* Reinforcement Learning
    + Google DeepMind (David Silver, Alex Graves, V. Mnih, Shanahan, Garnelo)
    + Oxford (Nando De Freitas, Shimon Whiteson)
    + Cambridge (Ghahramani)
    + Alberta University (Richard Sutton, Csaba Szepesvári)
    + UC Berkeley (Pieter Abbeel)
    + Carnegie Mellon (Salakhutdinov)
    + Imperial College (Murray Shanahan, Marta Garnelo)
    + University College London (Peter Dayan, Watkins, Dawid)
    + Edinburgh (Craig Innes)
    
----

* Cognitive Science
    + Carnegie Mellon (David Danks)
    + MIT (Josh Tenenbaum)
    + UC Berkeley (Tom Griffiths)
    + Edinburgh University (Andy Clark)
    + Göttingen Institut für Psychologie (York Hagmayer)
    + Max Planck Institute for Human Development (Bjorn Meder)

## Outer Circle
- Causality 

Reference                  | Summary                                |
------------------------   |------------------------                |
[@holland1986statistics]   | General review of different views on Causality |
[@heckerman1995decision]   |Decision-theoretic foundations for causal reasoning  |
[@spirtes2000causation]    | Classic General book on Causality      |
[@joyce1999foundations]    | Decision Theory and causal information |
[@pearl2009causality]      | Book on Causality and Graphical models |
[@koller2009probabilistic] | Book on Probabilistic Graphical Models |

----

- Reinforcement Learning 

Reference                    | Summary                                    |
-----------------------------|--------------------------------------------|
[@watkins1992q]              | Q-learning algorithm                       |
[@sutton1998reinforcement]   | Classic textbook on Reinforcement Learning |
[@szepesvari2010algorithms]  | State of the Art Algorithms                | 
[@van2012reinforcement]      | General review                             |
[@gershman2015reinforcement] | Reinforcement Learning in the Brain        |
[@li2017deep]                | Overview of Deep Reinforcement Learning    |

----

- Cognitive aspects

Reference            | Summary                                                                      |
---------------------|------------------------------------------------------------------------------|
[@krynski2007role]   | The role of causality in judgment under uncertainty
[@danks2014unifying] | Cognitive functions can be modelled as operations on graphical models        |
[@lake2017building]  | Conditions for machines to be considered intelligent: Causal learning is one |

## Middle Circle
Reference | Summary |
----------|---------|
[@dawid2002influence] | Influence Diagrams|
[@eberhardt2008causal] |Causal discovery as a game vs Nature|
[@hagmayer2009decision] | Choices as interventions |
[@wellen2012learning] | learning causal structure in local maner via precition-error loop |

----

Reference | Summary |
----------|---------|
[@mnih2013playing] | Introducing Deep Q-Network with Experience Replay |
[@silver2014deterministic] | Deterministic Policy Gradients |
[@lopez2015towards]        | Discriminative data-driven framework for distinguishing direction of causality |
[@DBLP:journals/corr/SchulmanLMJA15] | Trust Region Policy Optimization |
[@goudet2017learning] | Functional Causal Models with Generative Neural Networks |
[@o2016pgq] | Combining policy gradient and q-learning |

## Inner Circle
Reference | Summary |
----------|---------|
[@hagmayer2013repeated] | How repeated decision making is guided by causal beliefs. |
[@ortega2014generalized] | Thompson sampling to solve sequential adaptive control when policy is known. |
[@alon2015online] | When feedback is specified by graph, the graph structure controls the difficulty of learning. |
[@lattimoreNIPS2016] | Causal Bandits: Given a causal model, choose best intervention. |

----

Reference | Summary |
----------|---------|
[@albrecht2016exploiting] |Exploiting Causality for Selective Belief Filtering in Dynamic {B}ayesian Networks|
[@garnelo2016towards] | Reinforcement Learning framework that builds symbolic representation of environment. |
[@innes2018reasoning] | An agent which, starting unaware of factors on which an optimal policy depends, learns optimal behaviour for single-stage decision problems via direct experience and advice from a domain expert.| 
[@pearl2018theoretical] |Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution|

----
