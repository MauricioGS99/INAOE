\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish, mexico]{babel}
\usepackage{listings}
\usepackage{breakcites}
\usepackage{dsfont}
\usepackage{hyperref}
\usepackage{amssymb,amsthm,amsmath,latexsym}
\usepackage[margin=1.5cm]{geometry}
\usepackage{natbib}
%\bibliographystyle{stylename}
%\usepackage{fancyhdr}
%\pagestyle{fancy}
\theoremstyle{plain}
\newtheorem{teo}{Teorema}
\newtheorem{prop}[teo]{Proposición}
\newtheorem{defi}[teo]{Definición}
\newtheorem{obs}[teo]{Observación}
\newtheorem{lem}[teo]{Lema}
\newtheorem{cor}[teo]{Corolario}
\usepackage[pdftex]{color,graphicx}
\newcommand{\sgn}{\mathop{\mathrm{sgn}}}
\title{Research Problem}
\author{Mauricio Gonzalez Soto}
\begin{document}
%\nocite{*}
\maketitle
\section{Introducción}
El problema que estamos tratando de atacar consiste en descubrir relaciones causales que se encuentren presentes en diversos fenómenos. El enfoque que se propone es descubrir estas relaciones mediante \textit{interacción} con el entorno.\\
\\
\cite{lake2017building} argumentan que los sistemas inteligentes actuales, a pesar de sus impresionantes logros, aun están lejos de alcanzar una verdadera inteligencia; uno de los requisitos para esto es que puedan construir modelos causales que sean interpretables, y no sólo reconocer patrones estadísticos.\\
\\
 las intuiciones iniciales sobre aprendizaje causal por interacción provienen de estudios realizados en Psicología Computacional y Ciencias Cognitivas, de aquí se sabe que los seres humanos obtienen conocimiento causal al encontrarse en situaciones en las cuales deben tomar decisiones de manera secuencial para lograr un objetivo futuro. Se sabe también que el conocimiento causal adquirido puede ser utilizado para mejorar las futuras decisiones tomadas.\\
\\
Notando que los modelos causales tradicionales requieren de intervenciones y experimentación en el mundo, y que estas intervenciones son una forma de interacción y alteración del mundo, salta a la vista la relación entre Aprendizaje por Refuerzo y aprendizaje causal. Entonces, resulta natural preguntar cómo pueden aprenderse relaciones causales de manera interactiva de modo que el conocimiento causal adquirido permita a un agente tomar mejores decisiones. La pregunta se convierte entonces en cómo obtener e incorporar conocimiento causal en un problema de aprendizaje por refuerzo. Además, uno se pregunta si es posible utilizar este conocimiento causal obtenido al interactuar sobre el mundo para realizar otro tipo de inferencias causales en ciertos dominios de interés.

\section{Círculos del State of The Art}
\subsection{Outer Circle}
\begin{itemize}
\item (\cite{suppes1970probabilistic})
\item \cite{holland1986statistics}
\item \cite{sutton1998reinforcement}
\item Causal decision theory (\cite{joyce1999foundations}): Toma de decisiones bajo incertidumbre utilizando información causal; es decir; sé que $A$ \textit{causa} $B$, entonces puedo predecir qué pasará si $A$ es escogido, u observado.
\item (\cite{spirtes2000causation})
\item \cite{pearl2009causality},
\item Causal graphical models ( \cite{koller2009probabilistic}): Semántica causal sobre modelos gráficos probabilistas.
\item Libro de Teoría de la Decisión bajo incertidumbre (\cite{gilboa2009decision}).
\item \cite{pearl1988probabilistic}, Libro clásico sobre uso de grafos para representar distribuciones.
\item \cite{van2012reinforcement}, Estado del arte de aprendizaje por refuerzo.
\item  \cite{danks2014unifying} Representar modelos mentales como modelos gráficos probabilistas; en particular, el problema de inducción causal. 
\item \cite{gershman2015reinforcement}. Aprendizaje por Refuerzo en el Cerebro.
\item \cite{lopez2015towards}. Hacia una teoría de aprendizaje para relaciones causa-efecto.
\item \cite{lake2017building}. Condiciones que, idealmente, deberían cumplir algoritmos para que se pueda decir que piensan \textit{como} los humanos,
\end{itemize}
\subsection{Middle Circle}
\begin{itemize}
\item Decision problems and causal models: \cite{dawid2002influence} show how causal Bayesian Nets can be augmented with decision nodes as to model decision problems that use causal information.
\item \cite{hagmayer2009decision}
\item \cite{meder2009role}
\item \cite{meder2010observing}
\item \cite{kemp2010learning}
\item On-line learning in causal models(\cite{wellen2012learning})):  Causal learning in an on-line setting has been framed in terms of causal graphical models, where in a causal graphical model edges are added or removed in a prediction-error loop.

\item \cite{lopez2015towards}
\item \cite{krynski2007role} Human reasoning under uncertainty naturally operates over causa mental models and statistical data supports correct Bayesian inference only when they can be incorporated into a causal model. 
\item \cite{van2016deep}
\item \cite{goudet2017learning}
\item \cite{leibo2017multi}
\item \cite{hernandez2017survey}
\item \cite{li2017deep}. Overview general de Deep Reinforcement Learning

\end{itemize}
\subsection{Inner Circle}
\begin{itemize}
\item \cite{eberhardt2008causal} plantea el problema de \textit{descubrimiento causal} como un juego entre un científico vs la naturaleza en el cual la naturaleza intenta mantener ocultos sus secretos y el científico intenta descubrirlos mientras minimiza un costo. 
\item \cite{eberhardt2012number}
\item \cite{hauser2012two}
\item Causal learning through sequential decision making (\cite{hagmayer2013repeated}).
\item \cite{ortega2014generalized} 
\item \cite{bramley2015staying}
\item \cite{alon2015online} Estudian problemas de aprendizaje on-line en los cuales en cada ronda un jugador escoje una acción y recibe un \textit{loss} previamente especificado. Estudian las \textit{feedback graphs} que básicamente dicen si al ejecutar acción $i$ vemos el loss asociado a acción $j$ y estudian cómo la estructura de este grafo afecta el aprendizaje. Es principalmente un paper de estructura de grafos y el \textit{aprendizaje} se define en términos de Teoría de Juegos.
\item \cite{lattimoreNIPS2016} Quieren descubrir la mejor intervención de modo que se minimice el \textit{regret} después de $T$ rondas.
\item \cite{zhang2017deeper} Una vuelta a Experience Replay
\item \cite{albrecht2017autonomous}
\item \cite{albrecht2016exploiting}
\item \cite{garnelo2016towards}
\item \cite{pearl2018theoretical}
\item \cite{innes2018reasoning}. 
\end{itemize}

\section{Motivación}
Tomando el cuenta los trabajos mencionados en la sección anterior, surge la pregunta de si es posible diseñar un agente que aprenda a realizar una tarea por medio de interacción con su entorno y mediante el uso de información causal recolectada a través de esa interacción. Este agente buscará aprender un modelo causal de su entorno al mismo tiempo que aprende una \textit{política}. Con cada acción que toma, el agente aprenderá  un poco sobre la \textit{estructura causal} de su entorno y utilizará este conocimiento para la futura toma de decisiones.\\
\\
Cada decisión que el agente toma (acción que lleva a cabo) altera el estado del mundo. La información obtenida a partir de cada acción, será utilizada para actualizar el conocimiento causal que se tenga hasta el momento, esto con el fin mejorar las decisiones que toma y para aprender otros aspectos estructurales (causales) sobre su ambiente. La intuición detrás de esto es que al aprender mediante una toma de decisiones repetida, decisiones que intervienen directamente sobre el entorno, se aprenden también relaciones causales; por ejemplo, al aprender a conducir se aprende implícitamente que ciertos movimientos del volante \textit{causan} ciertos cambios en el estado del automovil (\cite{danks2014unifying}). Otro ejemplo es en el contexto de videojuegos, en el cual al tomar ciertas decisiones se conoce el impacto que estas provocan en el estado del juego, conocimiento que a su vez es utilizado para tomar mejores decisiones en un futuro y que maximicen las recompensas u objetivos a largo plazo. No sólo hay intuición detrás de esto, pues se sabe que los seres humanos utilizan el conocimiento causal que la experiencia les provee para realizar predicciones para intervenciones en el mundo no realizadas aun (\cite{meder2008inferring}, \cite{hagmayer2009decision})\\
\\
El trabajo de \cite{wellen2012learning} muestra cómo un modelo gráfico causal podría ser aprendido en línea al tomar en cuenta las decisiones tomadas y las recompensas obtenidas por un tomador de decisiones. Por otro lado, los modelos de \cite{sloman2006causal}, \cite{meder2008inferring}, \cite{hagmayer2009decision} y \cite{hagmayer2013repeated} muestran que estas intuiciones no son erradas y que los seres humanos incorporamos y adquirimos conocimiento causal al enfrentarnos en problemas de toma de decisión simples, aunque aun no existen muchos estudios en los cuales los escenarios de decisión no sean hipotéticos sino que se intente maximizar una recompensa real a largo plazo. De la misma manera, \cite{hagmayer2013repeated} mencionan que faltan estudios en donde las personas tengan otro objetivo más allá de revelar directamente la estructura causal del entorno y en los cuales vean directamente las consecuencias de sus acciones. Por lo tanto, es interesante preguntar si un agente que busque maximizar una recompensa pueda aprender modelos causales mediante la interacción con su entorno.
\\
Un agente que interactua con su entorno con el fin de maximizar una recompensa efectivamente puede aprender aspectos estructurales sobre el entorno, como nos muestra \cite{garnelo2016towards}, quien además señala que el agente logra mejorar su desempeño. Además, muestran la importancia de tener una semántica que permita hablar de la estructura del entorno. Ellos utilizaron lógica de primer orden, pues están más interesados en hacer razonamientos \textit{de alto nivel} sobre el entorno, que involocura más que las relaciones causales, y por esto ellos se comprometen con lógica de primer orden, que al restringirnos al caso causal se queda corta pues los modelos gráficos probabilistas son más expresivos, además de modulares y montónicos.\\
\\
Mientras que \cite{garnelo2016towards} utiliza una representación lógica para escoger una acción, \cite{lattimoreNIPS2016} explora la pregunta de cómo escoger \textit{la mejor intervención} dado un modelo causal a través de una serie de intervenciones y observaciones secuenciales; por ejemplo, en el caso en el que un granjero quiera maximizar sus cosechas, y sabe que ese rendimiento está afectado por el uso de fertilizante, la humedad y la exposición al sol, pero sólo tiene los recursos para llevar a cabo sólo una de estas modificaciones en cada temporada. El algoritmo de Lattimore le dirá, después de $T$ observaciones, cuál es la mejor intervención a llevar a cabo con el fin de maximizar esa cantidad. Este podría ser un primer paso hacia responder la pregunta \textit{¿dado un modelo causal, qué acción tomar?}
\\
Por otro lado, en el caso inverso \cite{ortega2014generalized} exploran un esquema de muestreo llamado Muestreo de Thompson, en el cual un agente muestrea una de entre varias acciones de acuerdo a la probabilidad subjetiva que tenga el agente de que esta sea la mejor acción. Los autores conjeturan que esto pueden utilizarse para hacer inducción causal sobre una serie de acciones establecidad (política conocida).

\section{Problema de investigación}
\subsection{Enunciado}
Aprendizaje de modelos causales a través de la interaccion con el entorno y el uso de este conocimiento para la toma de mejores decisiones en problemas de decisiones secuenciales.
\subsection{Relevancia}
Los métodos actuales de Aprendizaje por Refuerzo (RL) son puramente reactivos (\cite{garnelo2016towards}), además de necesitar de mucho tiempo de entrenamiento, pues los métodos usuales requieren de observar muchas parejas acción-estado (\cite{sutton1998reinforcement}). Las representaciones del entorno normalmente utilizadas, en términos de Procesos de Decisión Markovianos no permiten hacer razonamiento de alto nivel sobre este entorno (\cite{innes2018reasoning}, \cite{garnelo2016towards}), por lo que aprender estructuras causales permitiran a un agente tomar mejores decisiones pues podrá considerar consecuencias de sus acciones en vez de sólo estar reaccionando a recompensas.
\subsection{Factibilidad}
Del trabajo desarrollado en el área de RL sabemos que es posible que un agente autónomo aprenda una enorme variedad de tareas mediante interacción; ejemplos de esto los tenemos en los trabajos de \cite{mnih2013playing}, \cite{mnih2015human}, \cite{silver2016mastering}, \cite{silver2017mastering}, \cite{impala2018}. \\
\\
Por otro lado, sabemos que los seres humanos adquieren conocimiento causal de su entorno en el caso de problemas de decisión secuenciales (\cite{hagmayer2009decision}, \cite{hagmayer2013repeated}).\\
\\
Además, gracias al trabajo de \cite{lattimoreNIPS2016} se sabe que es posible escoger acciones dado un modelo causal.\\
\\
Por lo tanto, los diversos elementos existen (en sus ideas de fondo) y pueden ser unidos, aunque no será una tarea trivial. 
\section{Estado del arte}
\subsection{Grupos principales}
\subsubsection{Causalidad}
\begin{itemize}
\item Carnegie Mellon (Spirtes)
\item UC Los Angeles (Pearl)
\item MIT (Josh Tenenbaum)
\item Max Planck Institute for Intelligent Systems (Schölkopf)
\item University of Copenhagen  (Jonas Peters)
\item Edinburgh (Albrecht, Subramanian Ramamoorthy)
\end{itemize}
\subsubsection{Reinforcement Learning}
\begin{itemize}
\item Google DeepMind (David Silver, Alex Graves, V. Mnih, Murray Shanahan, Marta Garnelo)
\item Oxford (Nando De Freitas, Shimon Whiteson)
\item Cambridge (Ghahramani)
\item  Alberta University (Richard Sutton, Csaba Szepesvári)
\item UC Berkeley (Pieter Abbeel)
\item Carnegie Mellon (Salakhutdinov)
\item Imperial College (Murray Shanahan, Marta Garnelo)
\item University College London Gatsby Unit (Peter Dayan, Watkins, Dawid)
\item Edinburgh (Craig Innes)
\end{itemize}
\subsubsection{Ciencias Cognitivas}
\begin{itemize}
\item Carnegie Mellon (David Danks)
\item MIT (Josh Tenenbaum)
\item UC Berkeley (Tom Griffiths)
\item Edinburgh University (Andy Clark)
\item Göttingen Institut für Psychologie (York Hagmayer)
\end{itemize}
\newpage
\bibliographystyle{apalike}
\bibliography{/Users/MauricioGS1/INAOE/Segundo_Semestre/Propuesta/Bibliografia.bib}
\end{document}