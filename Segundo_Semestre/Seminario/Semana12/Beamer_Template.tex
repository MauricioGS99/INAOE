\documentclass{beamer}

\mode<presentation> {


\usetheme{Madrid}

}
\usepackage{booktabs}
\usepackage[utf8]{inputenc}
\usepackage[spanish, mexico]{babel}
\usepackage{listings}
\usepackage{breakcites}
\usepackage{dsfont}
\usepackage{hyperref}
\usepackage{amssymb,amsthm,amsmath,latexsym}
\usepackage{natbib}
\theoremstyle{plain}
\newtheorem{teo}{Teorema}
\newtheorem{prop}[teo]{Proposición}
\newtheorem{defi}[teo]{Definición}
\newtheorem{obs}[teo]{Observación}
\newtheorem{lem}[teo]{Lema}
\newtheorem{cor}[teo]{Corolario}
\usepackage{tikz}
\usetikzlibrary{trees}
\usetikzlibrary{calc}


%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Short title]{Use and acquisition of causal knowledge in a decision making process} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{Mauricio Gonzalez Soto} % Your name
\institute[INAOE] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Instituto Nacional de Astrofísica Óptica y Electrónica \\ % Your institution for the title page
\medskip
\textit{mauricio@inaoep.mx} % Your email address
}
\date{\today} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Overview} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

%------------------------------------------------
\section{Introduction} 
\begin{frame}
\frametitle{Introduction}
\begin{itemize}
\item Given the enormous complexity in our environment, we are constantly faced with making decision in uncertain environments.
\item Causal relations that hold in our environment allows to predict consequences.
\item “Causality” evokes certain regularities, we want to think about the causes of a disease in order to avoid it. 
\item Human beings tend to find causal narratives, sometimes excesively and that leads to incorrect judgements (\cite{tversky1977causal}, \cite{tversky1980causal}, \cite{kahneman2011thinking}).
\item How to effectively use causal knowledge to make decisions?
\end{itemize}
\end{frame}
	\subsection{Motivation} 
	\begin{frame}
	\frametitle{Motivation}
	\begin{itemize}
	\item In decision making processes with a concrete goal to be achieved, causal knowledge is correctly used by human beings (\cite{sloman2006causal}, \cite{nichols2007decision}, \cite{meder2010observing}, \cite{hagmayer2013repeated}, \cite{danks2014unifying}).
	\item Although they sometimes violate some of the axioms required in the formalization of causality (\cite{rottman2014reasoning}).
	\item \cite{lattimoreNIPS2016} show that an autonomous agent can choose good actions if it uses causal information.
	\end{itemize}
	\end{frame}
	\subsection{Justification}
	\begin{frame}
	\frametitle{Justification}
	\begin{itemize}
	\item Human beings conceive their actions upon the world as \textit{interventions} (\cite{hagmayer2009decision}).
	\item Following this idea, \cite{lattimoreNIPS2016} consider a decision making process where available actions are interventions in a causal graphical model. Faster identification of best-action.
	\item They require the full model to be known, how to acquire and use causal info on the fly? 
	\item This is, in part, how human beings work (\cite{hagmayer2013repeated}).
	\item Human beings learn local aspects and then unify them in a coherent story (\cite{fernbach2009causal}, \cite{waldmann2008causal}, \cite{wellen2012learning}).
	\end{itemize}
	\end{frame}
\section{Framework}
	\subsection{Causal Relations}
		\subsubsection{Definition of Causality}
		\begin{frame}
		\frametitle{So, what is causality?}
		\begin{itemize}
		\item Subject of heavy philosophical debate.
		\item Statisticians have some kind of fear about using this word.
		\item Economists try to find assumptions under which an econometric model is causal.
		\item Many definitions exists, (see for example \cite{arlo2016readings}).
		\item Our attitude here is the expressed by \cite{danks2013functions}, \cite{danks2014unifying}:\\
		\textit{...all that matters is that our concept appropriately tracks some relation(s) in the world that can ground and support the learning, reasoning and inferences that we do with our causal knowledge...}
		\end{itemize}
		\end{frame}
		\begin{frame}
		\frametitle{Definition of Causality}
		The working definition of Causality that we will be using is based upon \cite{spirtes2000causation}
		\begin{defi}
		Let $(\Omega, \mathcal{F}, \mathbb{P})$ a finite, probability space and consider a binary relation  $\to  \subseteq \mathcal{F} \times \mathcal{F}$ such that
		\begin{itemize}
		\item Transitive: if $A \to B$ and $B \to C$ for $A,B,C \in \mathcal{F}$ then $A \to C$.
		\item  Irefflexive: for each $A \in \mathcal{F}$ it is not true that $A \to A$.
		\item Antisymetric: For $A, B \in \mathcal{F}$, if $A \to B$, then it is not true that $B \to A$.
		\end{itemize}
		If $A \to B$ we will say that $A$ is a cause of $B$.
		\end{defi}
		\end{frame}
		\subsubsection{Direct and indirect causes}
		\begin{frame}
		\frametitle{Direct and Indirect Causes}
		Consider a refinement\footnote{In the sense as a refined topology} $\mathcal{F}_s$ of $\mathcal{F}$ and extend “$\to$” properly. Notice that it could happen that if $A \to B$, then for some $C_s \in \mathcal{F}$ we have that $A \to C_s \to B$. 
		\begin{defi}
		Under the same setup as the previous definition, we say that $A$ is a direct cause of $B$ if $A \to B$ and if it does not exist any $C$ such that $A \to C \to B$ for all proper refinements $\mathcal{F}_s$ of $\mathcal{F}$.
		\end{defi}
		\end{frame}
		\subsubsection{Causal Graph Construction}
		\begin{frame}
		\frametitle{From Causes to Graphs}
		For each pair $A,B \in \mathcal{F}$ such that $A \to B$ and $A$ is a direct cause of $B$, define a node representing each variable, and a directed edge joining them.
		\begin{prop}
		Given a binary relation “$\to$” that satisfies previous requirement, let $\mathcal{G}$ be the directed graph that is obtained by the previous procedure. Then, $\mathcal{G}$ contains no cycles.
		\end{prop}
		\begin{proof}
		Consider a cycle $A \to C_1 \to C_2 \to \cdots \to A$, then we would have that $A \to A$, which contradicts the Irreflexive property.
		\end{proof}
		\end{frame}
		\begin{frame}
		\begin{obs}
		Notice that if the graph is finite, then necesarily there will be nodes without parents, which in the context of causal models will be called exogenous variables
		\end{obs}
		\end{frame}
		\subsubsection{Probability Measure built from the Graph}
		\begin{frame}
		\frametitle{from Graphs to Probability Measures}
		Given a Directed Acyclic Graph, one can build the probability distribution $P_\mathcal{G}$ that expresses the relations of the graph. We require that this measure satisfies:
        \begin{itemize}
        \item Causal Markov Condition.
        \item Causal Minimality.
         \item Causal Faithfulness.
\end{itemize}
This requirements will be treated in an axiomatic fashion.
		\end{frame}
	\subsection{Causal Graphical Models}
	\begin{frame}
	\frametitle{Causal Graphical Models}
	\begin{defi}
	A causal graphical model consists of:
	\begin{itemize}
	 \item A set of random variables $\mathcal{X}=\{ X_1,...,X_n \}$, 
	 \item A directed acyclic graph $\mathcal{G}$
	 \item An operator $do()$ defined over the space of DAG's whose action consists of: given $\mathbf{X} \subseteq \mathcal{X}$ and $\mathbf{x} = \{ x_{i_1}, x_{i_2}, ... , x_{i_j} \} \in Val(\mathcal{X})$, then $do(\mathbf{X} = \mathbf{x} )$ assigns to each $X_j \in \mathbf{X}$ the value $x_{i_j}$ and deletes any incoming edge on it.
	 \end{itemize}
	 \end{defi}
	 \end{frame}
		\subsubsection{The identifiability issue} 
		\begin{frame}
		\frametitle{Identifiability}
		When can causal questions be answered using only observational data?
		\end{frame}
		\subsubsection{Do Calculus}
		\begin{frame}[allowframebreaks]
		\frametitle{Do calculus}
		The do calculus is a set of rules which allow, under certain conditions, to express interventional statements in terms of observational statements.
		\end{frame}
		\begin{frame}
		\frametitle{Some results}
		\begin{teo}{\cite{peters2017elements}}\\
 		The next statements hold:
		\begin{itemize}
		\item The do-calculus is complete; i.e., for all identifiable intervention there exists a sequential application of the rules that enables one to compute such intervention (\cite{huang2006pearl}, 				\cite{shpitser2006identification}).
		\item There is an algorithm capable of finding all identifiable interventional distributions (\cite{tian2002}, \cite{huang2006pearl}).
		\item There exits a necessary and sufficient graphical criterion for identifiability of interventional distributions (\cite{shpitser2006identification}, \cite{huang2006pearl}).
		\end{itemize}
		\end{teo}
		\end{frame}
		\begin{frame}
		\frametitle{Answering the identifiability problem}
		\begin{cor} 
		A distribution $q=P(y_1,...,y_k | do(x_1),...,do(x_n))$ is identifiable in a causal graphical model $\mathcal{G}$ if there exists a finite sequence of transformations, where each of them corresponds to one of the simplification rules of Theorem \ref{docalculus} which reduces $q$ to a probability statement conditioned only on observations (i.e. the $do$ operator doesn't appear).
\end{cor}
		\end{frame}
	\subsection{Decision Theory}
	\begin{frame}
	\frametitle{Decision Theory: some classic results}
	\end{frame}
		\subsubsection{von Neumann-Morgenstern's Theorem}
		\begin{frame}
		\frametitle{von Neumann-Morgenstern's Theorem: existence of a utility function}
		Let $X$ be a finite set of outomes and consider $\mathcal{L}$ the set of lotteries over elements of $X$ and consider a preference relation $\succeq$ over $\mathcal{L}$.
		\begin{teo}
		A preference relation $\succeq$ over lotteries satisfies the (not shown here) axioms of rational choice if and only if there exists a function $u: X \to \mathbb{R}$ such that for all pair of lotteries $P,Q \in \mathcal{L}$ we have that $P \succeq Q$ if and only if 
		\[ \sum_{x \in X} P(x) u(x) \geq \sum_{x \in X} Q(x) u(x).  \]
		\end{teo}
		\end{frame}
		\subsubsection{De Finetti's Theorem}
		\begin{frame}
		\frametitle{De Finetti's Theorem: existence of subjective probability}
		Consider $n$ states of the world and consider $X$ the set of \textit{bets} over this set, where a bet is an $n$-dimensional vector whose $i$-th entry corresponds to the monetary amount that is being bet on state $n$. Consider a preference relationship $\succeq$ over the $X$. Also, some axioms are required over $\succeq$
		\begin{teo}
		The relationship $\succeq$ satisfies the axioms if an only if there exists a probability vector of size $n$ such that for all $x,y \in X$,
		\[ x \succeq y \textrm { if and only if } p^t x \geq p^t y. \]
		\end{teo}
		\end{frame}
		\subsubsection{Savage's Theorem}
		\begin{frame}
		\frametitle{Savage's Theorem}
		vNM Theorem requires probabilities and guarantees the existence of utility. DeFinetti's Theorem requires utility and guarantees the existence of probability...where do we start? Savage's Theorem gives both in tandem, but requires more abstract (and criticizable) axioms.
		\begin{teo}
		Consider a finite set $X$ of outcomes and a set $S$ of states.  A preference relationship $\succeq$ over $F=X^S$ satisfies the axioms if and only if there exists a finite non-atomic measure $\mu$ defined over the measurable space $(S, 2^S)$ and a non-constant function $u :X \to \mathbb{R}$ such that for all $f,g \in F$ it holds that
		\[ f \succeq g \textrm{ if and only if} \int_S u(f(s)) d \mu(s) \geq \int_S u(g(s)) d \mu(s). \]
		\end{teo}
		\end{frame}
		
	\subsection{Game Theory}
\begin{frame}
\frametitle{Game Theory}
Game Theory is an area of Mathematics that is used to model strategic interaction situations, which can be interpreted as multi-agent decision problems, where the outcomes for each player depend on the actions of the other players.\\
Aumann defined Game Theory as \textit{...optimal decision making in the presence of others with different objectives...}\\
 Several classes exist:
\begin{itemize}
\item Strategic Games
\item Extensive Games
\item Stochastic Games
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Strategic Games}
Consider a situation where two or more agents interact and each one of them seeks to fullfill an objective. Here, each agent will take one and only one decision and simultaneously with other agents.
\begin{defi}{A strategic normal form game (\cite{osborne1994course}, \cite{shoham2008multiagent}) }\\
A finite normal form game is a tuple $(N,A,u)$ where:
\begin{itemize}
\item $N$ finite set of players.
\item $A= A_1 \times \cdots \times A_n$ each $A_i$ are the actions available to player $i$. Each element  $a = (a_1,...,a_n) \in A$ es lis called an action or strategy profile.
\item For each player $i \in N$, a preference relation $\succeq_i$ defined over $A$.
\end{itemize}
\end{defi}
\end{frame}

\begin{frame}
\frametitle{Bayesian Games}
\begin{defi}{\label{juegobayesiano}}
A strategic normal form Bayesian game is composed by:
\begin{itemize}
\item finite set of players $N$
\item Set of states $\Omega$.
\item For each player: 
\begin{itemize}
\item Set of possible actions $A_i$,  and define $A = A_1 \times \cdots \times A_n$.
\item A finite set $T_i$ of observable signals for player $i$.
\item Signal function $\tau_i : \Omega \to T_i$.
\item A probability measure $P_i$ defined over $\Omega$ such that $P( \tau^{-1}_i (t_i)) >0$ for all  $t_i \in T_i$.
\item Preference relation $\succeq_i$ defined over  $\mathcal{M}(A \times \Omega)$
\end{itemize}
\end{itemize}
\end{defi}
\end{frame}

\begin{frame}
\frametitle{Bayesian games: interpretation}
\begin{itemize}
\item $\Omega$ contains all of the descriptions of players' relevant characteristics.
\item Each player has prior beliefs over this characteristics, represented by $P_i$.
\item At any given play, $\omega \in \Omega$ is realized.
\item Each player observes $\tau_i(\omega)$ and let $T_i$ all posible values of $\tau_i$.
\item Player $i$ receives $t_i$, so he  deduces that the true state is in the set $\tau_i^{-1}(t_i)$.
\item So he updates the probability asigned to $\omega \in \Omega$ by $P_i(\omega) / P(\tau_i^{-1}(t_i))$.
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Perfect Information Extensive Games}
\begin{defi}
A perfect information extensive game:
\begin{itemize}
\item A finite set  $N$ of players.
\item Set $A$ of actions.
\item Set $H$ of sequences (called histories) such that:
	\begin{itemize}
	%	\item The empty sequence belongs to $H$.
		\item If $(a_k)_{k=1}^K \in H$ ($K$ can be infinite) and $L < K$ then $(a_k)_{k=1}^L \in H$.
		\item If $(a_k)_{k=1}^\infty$ satisfies $(a_k)_{k=1}^L \in H$ for all $L$ then $(a_k)_{k=1}^\infty \in H$
	\end{itemize}
\item If $(a_k)_{k=1}^K \in H$ where $K$ is finite and if no $K+1$ exists such that $(a_k)_{k=1}^{K+1} \in H$ then the sequence is called terminal. The set of terminal histories is called $Z$
\item A function $\chi : H \to 2^A$  that assigns a set of possible actions to be taken after a history $h$ has ocurred.
\item A function $\rho: H \to N$ that assigns to every nonterminal history a player $\rho(h) \in N$ who will take an action after history $h$ has ocurred.
\item For each player, a preference relation $\succeq_i$.
\end{itemize}
\end{defi}
\end{frame}

\begin{frame}
\frametitle{Imperfect Information Extensive Games}
\begin{itemize}
\item $(N,A,H,Z,\chi,\rho,\sigma, u)$ is a perfect information extensive game.
\item $I=(I_1,...,I_n)$for each player $i$, we have that  $I_i = \{ I_{i,1}, I_{i,2}, ... , I_{i,k_i} \}$ is a partition of $\{ h \in H: \rho(h)=i \}$ that satisfies that if $\chi(h)=\chi(h')$ then there exists $j$  such that $h \in I_{i,j}$ and $h \in I_{i,j}$.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Repeated Games}
Playing repeatedly a \textit{base game}: this is interesting because if the sum of payments is considered in the long run, then the one-step equilibria can change. For example, in the Prisoner Dilemma, players may realize that the one-stage Nash equilibrium is not optimal: players can \textit{learn}.
\end{frame}

\begin{frame}
\frametitle{Learning in Games}
In a repeated game, what can a rational agent learn? After several rounds he can learn the strategy of others and use it, or play a certain strategy to deceive others that are learning. 
\end{frame}

\begin{frame}
\frametitle{Rational Learning}
Let $S$ be the set of the opponents considered by player $i$ and $H$ the stories of the game. Then, the probability assigned by player $i$ to the event in which the opponent is playing a particular strategy $s_{-i} \in S^i_{-i}$ given that story $h$ is observed is:
\[ P_i(s_{-1} | h) = \frac{P_i(h|s_{-i})P_i(s_{-i})}{\sum_{s'_{-i}} P_i(h | s'_{-i})P_i(s'_{-i})}. \]
\end{frame}
\begin{frame}
\frametitle{Rational Learning and Belief accuracy}
\begin{teo}
Let $s$ be a repeated game strategy profile  for a given $n$ player game and let $P=P_1,...,P_n$ be a tuple of probability distributions over such strategy profiles ($P_i$ are player $i$'s beliefs). Let $\mu_s$ be de distribution over infinite game histories induced by the profile $s$ and let $\mu_P$ the distribution induced by the belief tuple $P$. Then, if
\begin{itemize}
\item At each round each payer $i$ plays a best-response strategy given his beliefs $P_i$.
\item After each round player $i$ updates $P_i$ using rational updating.
\item $\mu_s$ is absolutely continuous with respect to $\mu_p$.
\end{itemize}
Then, for all $\varepsilon > 0$ and for almost every history in the support of $\mu_s$ there is a time $T$ such that for all $t \geq T$ the play $\mu_{P_i}$ predicted by player $i$'s beliefs is $\varepsilon$-close to the distribution of play $\mu_s$ predicted by the actual strategies.
\end{teo}
\end{frame}
\begin{frame}
This means that a players' beliefs will converge to the truth if he is using Rational Updating and playing a Best Response.
\end{frame}
\section{Research Question}
\begin{frame}
\frametitle{Research Question}
In an uncertain environment where the relationship between events is governed by some causal mechanism, how can an intelligent agent acquire and use causal knowledge from his interaction with the environment in a decision making proces in order to learn faster to choose good actions?
\end{frame}

	\subsection{Hypothesis}
	\begin{frame}
	\frametitle{Hypothesis}
Let $\mathcal{G}$ a causal graphical model whose probability distribution $P_\mathcal{G}$ satisfies (the axioms). Let $(D,E,c)$ a decision problem whose uncertain events are related to the variables in $\mathcal{G}$. Then, if a decision maker doesn't know the probabilities of the uncertain events in $E$,  by repeatedly making decisions he can learn causal information and use it to find the optimal actions (in the sense of expected utility theory) in less rounds than if he doesn't consider causal information.
	\end{frame}

\section{Set up and modelling}
\begin{frame}[allowframebreaks]
\frametitle{Modelling}
\begin{itemize}
\item A (one-stage) decision problem under uncertainty consists in a set of actions, a set of consequences and a set of uncertain events.
\item The standard interpretation (\cite{bernardo2000bayesian}, \cite{gilboa2009decision}) is that an agent will choose some action, then an uncertain event will occur and this will determine the outcome.
\item If all probabilities and utilities of outcomes are known, one can choose by maximizing MEU.

\end{itemize}
\end{frame}
\begin{frame}
\begin{itemize}
\item Now, consider an environment where the relation between the actions, the events, and the consequences is given by a causal model.
\item If this model is known by a decision maker, then it can be used for determining the probabilities of effects given causes (outcomes given actions).
\item If the model is not known, we want to learn it by interaction.
\end{itemize}
\end{frame}
\begin{frame}
\begin{itemize}
\item Interaction will be modelled as a \textit{game} between two players: the original decision maker, and a new abstract entity called Nature.
\item Nature will pick its actions from the causal model.
\end{itemize}
\end{frame}
\begin{frame}
\begin{itemize}
\item If the agent doesn't know the causal model, we must give him some way to reason about it.
\item The agent will have \textit{beliefs} about the causal model which will be used as a \textit{personal} causal model at any given time.
\item Beliefs will be updated from what is observed in each game.
\item Updating must be done in some \textit{consistent} (consistency is a statistical definition for estimators that converge to the true parameter.)
\end{itemize}
\end{frame}


\section{Previous work}
	\subsection{Optimal Interventions}
	\begin{frame}[allowframebreaks]
	\frametitle{Identifying Optimal Interventions}
	\begin{itemize}
		\item \cite{lattimoreNIPS2016} study the problem of decision making under uncertainty in causal models where the causal model is completely known.
		\item In their setup, the actions or choices available to dhe decision maker are interventions in the model.
		\item They look for the minimal-regret intervention that maximices some reward variable. 
		\item They show that optimal intervention can be found faster than if no causal info was considered. Open question of what to do when the causal graph is unknown
		\item Later, \cite{sen2017identifying} start with the same setting than in \cite{lattimoreNIPS2016} but they consider the case when not all of the graph is known. 
		\item They also find the optimal intervention over some variable in the unknown part of the model. They improve the results of \cite{lattimoreNIPS2016} in experiments, but they still require knowing conditional distributions.
	\end{itemize}
\end{frame}

\section{Objectives}
	\subsection{General Objective}
	\begin{frame}
	The proposed research has as a general objective: 
	\begin{itemize}
	\item Find the way in which an agent facing an uncertain environment which governed by a causal model can acquire and make use of causal knowledge in order to make good choices and improve his performance when compared to non-causal guided ways of choosing.
	\end{itemize}
	\end{frame}
	\subsection{Specific Objectives}
	\begin{frame}[allowframebreaks]
	\frametitle{Specific Objectives}
	\begin{itemize}
	\item To find a way to specify a game that captures the essential aspects of a decision making problem. This means that the strategies and equilibria of the game must be in correspondence with optimal solutions of the decision making problem.
\item To find a way to specify beliefs about causal structure; i.e., find (or build) a family of parametric distributions that represent in a coherent way beliefs or previous knowledge about the causal structure of some environment.
\item A way to specify initial beliefs according to past knowledge in a coherent fashion, previous work in \cite{billot2005probabilities}. Not any belief can be modelled by Bayesian priors  \cite{gilboa2016ambiguity}.
\item Theoretical guarantees of learning in this class of games.
\item To find a way of effectively using causal knowledge that the agent has at a given moment in order to make a good choice relative to the max utility.
\item Develop an update criterion for the beliefs of the agent based upon what he has observed from the environment while interacting. This updating must be achieved in a way that the sequence of models converges to the true model. In the Bayesian Non parametric setting this is known as \textit{consistency} (\cite{ghosal2017fundamentals}). Also, the beliefs obtained must be coherent with the decision making since we will be using those results. 
\item A way to specify a game strategy given past games and current beliefs.
\item An aproximate soluton criteria: maybe we can't wait until convergence, so what can be done with the current knowledge.
\end{itemize}
	\end{frame}
	
\section{Methodology}
\subsection{Working methodology}
\begin{frame}[allowframebreaks]
\frametitle{Working Methodology}
\begin{itemize}
\item Fix the class of games will be used to model the interactions between a decision maker and his environment.
\item Define what are the mentioned beliefs and how to represent them using probability distributions.
\item Update criteria that must converge to the true causal model.
\item Theoretical guarantees over the conditions and the update criteria.
\item Analyze, experiment and compare in three cases:
\begin{itemize}
	\item Knowing all of the model: how the chosen decision compares to best-action identification algorithm?
	\item Knowing only the structure of the model: using in each round the current knowledge as the “true” model and choosing with it.
	\item Not knowing anything: choosing-observing-updating cycle, compare number of rounds for utility above certain threshold
\end{itemize}
\end{itemize}
\end{frame}


\section{Preliminary Results}
\begin{frame}
\frametitle{Preliminary Results}
For the preliminary results we set up a toy example (the same one that we have been using) and analyze 3 separate cases in order of growing difficulty:
\begin{itemize}
\item In the first case, the decision maker knows it all.
\item In the second case, the decision maker only knows for certain the structure of the model, but not its parameters so he has beliefs about them.
\item In the third case, the decision maker knows nothing at all, so he has general beliefs about the causal model itself.
\end{itemize}
\end{frame}


\section{Limitations and possible extensions}
\begin{frame}
\frametitle{Limitations}
\begin{itemize}
\item We are considering a rational decision maker.

\item Causality is used as defined above and it is not questioned if it suits correctly realistic situations. We do not consider if other possible definitions of causality correspond better to decision making.

\item Nature has not intentions, but since we are giving her equal payments this could be modified later.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Possible extensions}
\begin{itemize}
\item Other forms of decision making, behavioral approach, bounded rationality.
\item Longer games of unknown structure.
\item More players besides nature.
\item Nature has intentions.
\item Game solutions (equilibria) in terms of the structure of the causal graphical model.
\item Dynamica systems associated with the learning process.
\end{itemize}
\end{frame}


\begin{frame}[allowframebreaks]
\frametitle{References}
\bibliographystyle{apalike}
\bibliography{/Users/MauricioGS1/INAOE/Segundo_Semestre/Propuesta/Bibliografia.bib}
\end{frame}

%------------------------------------------------

\begin{frame}
\Huge{\centerline{The End}}
\end{frame}

%----------------------------------------------------------------------------------------

\end{document}