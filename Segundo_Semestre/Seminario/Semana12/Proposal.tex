\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish, mexico]{babel}
\usepackage{listings}
\usepackage{breakcites}
\usepackage{dsfont}
\usepackage{hyperref}
\usepackage{amssymb,amsthm,amsmath,latexsym}
\usepackage[margin=1.5cm]{geometry}
\usepackage{natbib}
%\bibliographystyle{stylename}
%\usepackage{fancyhdr}
%\pagestyle{fancy}
\theoremstyle{plain}
\newtheorem{teo}{Teorema}
\newtheorem{prop}[teo]{Proposición}
\newtheorem{defi}[teo]{Definición}
\newtheorem{obs}[teo]{Observación}
\newtheorem{lem}[teo]{Lema}
\newtheorem{cor}[teo]{Corolario}
\usepackage[pdftex]{color,graphicx}
\usepackage{tikz}
\usetikzlibrary{calc}
%\newcommand{\sgn}{\mathop{\mathrm{sgn}}}
\title{Adquisición y uso de relaciones causales para la toma de decisiones bajo incertidumbre.}
\author{Mauricio Gonzalez Soto}
\begin{document}
%\nocite{*}
\maketitle
\section{Introducción}
	\subsection{Motivación}
	\subsection{Justificación}

\section{Marco Teórico}
	\subsection{Relaciones Causales}
		\subsubsection{Definición de Causalidad}
		La definición de Causalidad que estamos considerando es la que aparece en \cite{spirtes2000causation} y que consiste en lo siguiente:
		\begin{defi}
		Dado un espacio de probabilidad $(\Omega, \mathcal{F},\mathbb{P})$, consideramos una relación binaria $\to$ definida sobre elementos de $\mathcal{F}$ que sea
		\begin{itemize}
		\item Transitiva: si $A \to B$ y $B \to C$ para $A,B,C \in \mathcal{F}$ entonces $A \to C$.
		\item Irefflexiva: Para todo $A \in \mathcal{F}$ no es cierto que $A \to A$.
		\item Antisimétrica: Para $A, B \in \mathcal{F}$, si $A \to B$, entonces no se cumple que $B \to A$.
		\end{itemize}
		\end{defi}
		\subsubsection{Causas Directas e Indirectas}
		En términos de la definición anterior, si $A \to B$ entonces decimos que $A$ es una \textit{causa} de $B$ y que $B$ es el efecto de la ocurrencia de $A$. Consideremos ahora un \textit{refinamiento} $\mathcal{F}_s$ de la sigma-álgebra $\mathcal{F}$ de modo que ahora puedan existir nuevos eventos $C_s \in \mathcal{F}_s$ tales que $A \to C_s \to B$. Decimos que $A$ es una \textit{causa directa} de $B$ si se cumple que $A \to B$ y no existe evento $C$ tal que $A \to C \to B$ para todo refinamiento propio de $\mathcal{F}$. Notemos que la definición depende del conjunto de variables, por lo que al decir que un evento es causa directa de otro, es siempre relativo al conjunto de variables. 
		
		\subsubsection{Construcción de un grafo que represente relaciones causales}
		Abusando de la notación, podemos construir un grafo que represente las relaciones causales entre eventos de $\mathcal{F}$ de la siguiente manera: si $A \to B$ y $A$ es una causa directa de $B$, entonces se añade un nodo que representa el evento $A$ y un nodo que representa el evento $B$ y una arista dirigida que los une en el sentido de la causa hacia el efecto. A partir de definición de relación causal se obtiene un grafo acíclico dirigido. Además, requerimos las siguientes condiciones que relacionan el grafo así construido con la distribución de probabilidad $P_{\mathcal{G}}$ que éste expresa:
		\begin{itemize}
		\item Markov Causal
		\item Minimalidad Causal
		\item Fidelidad Causal
		\end{itemize}
		\subsubsection{Modelos gráficos causales}
		Un modelo gráfico causal consiste en un conjunto de variables aleatorias $\mathcal{X}=\{ X_1,...,X_n \}$, un grafo $\mathcal{G}$ cuyos nodos corresponden a variables en $\mathcal{X}$ y las aristas entre ellos a causas directas. Las variables de $\mathcal{X}$ están divididas en dos subconjuntos: las variables endógenas y las variables exógenas, en donde estas últimas son aquellas que no tienen padres en el grafo. Además, un operador $do()$ que está definido sobre grafos y cuya acción corresponde a lo siguiente: Dado $\mathbf{X} \subseteq \mathcal{X}$ y $\mathbf{x} = \{ x_{i_1}, x_{i_2}, ... , x_{i_j} \} \in Val(\mathcal{X})$, $do(\mathbf{X} = \mathbf{x} )$ consiste en asignar a cada $X_j \in \mathbf{X}$ el valor $x_{x_{i_j}}$ y eliminar todas las aristas que entran al nodo correspondiente a $X_i$ en el grafo $\mathcal{G}$.
		\subsubsection{El problema de la identificabilidad}
		
		\subsubsection{Cálculo Do}
		El cálculo do (\cite{pearl1995causal}, \cite{pearl2009causality}) consiste en un conjunto de reglas de inferencia que permiten manipular enunciados sobre observaciones que provienen de 		intervenciones. Consideremos un modelo gráfico causal $\mathcal{G}$ y sean $X,Y,Z$ conjuntos disjuntos de nodos de $\mathcal{G}$. Como notación, $\mathcal{G}_{\bar{X}}$ será el grafo que 	se obtiene al eliminar de $\mathcal{G}$ todas las aristas que entran en elementos de $X$; análogamente, $\mathcal{G}_{\underline{X}}$ será el grafo que se obtiene al eliminar de $\mathcal{G}$ todas las aristas que salen de elementos de $X$ y por último, $\mathcal{G}_{\underline{Z}\bar{X}}$ al eliminar arcos que entran de uno y que salen de otro conjunto.\\
	\\
		Las reglas de inferencia consisten en lo siguiente:
		\begin{teo}{\label{docalculus}} (\cite{pearl2009causality})\\
		Sea $\mathcal{G}$ un modelo gráfico causal y $P_{\mathcal{G}}$ la medida de probabilidad inducida por el modelo; entonces, para cualesquiera conjuntos disjuntos de variables $X,Y,Z,W$ se cumple que
		\begin{itemize}
		\item Si en el grafo $\mathcal{G}_{\bar{X}}$ se cumple que $Y$ es condicionalmente independiente de $Z$ dados $X$ y $W$, entonces
		\[ P_{\mathcal{G}}(Y=y | do(X=x), Z=z, W=w) = P_{\mathcal{G}}(Y=y | do(X=x), W=w). \]
		\item Si en el grafo $\mathcal{G}_{\bar{X}\underline{Z}}$ se cumple que $Y$ es condicionalmente independiente de $Z$ dados $X$ y $W$, entonces
		\[ P(Y=y | do(X=x), do(Z=z), W=w) = P(Y=y | do(X=x), Z = z, W=w). \]
		\item Sea $Z(W)$ el conjunto de nodos en $Z$ tales que no son ancestros de ningún nodo en $W$ en el grafo $\mathcal{G}_{\bar{X}}$. Si en el grafo $G_{\bar{X}, \bar{Z(W)}}$ se cumple que $Y$ es condicionalmente independiente de $Z$ dados $X$ y $W$ entonces,
		\[ P(Y=y | do(X=x), do(Z=z), W=w) = P(Y=y | do(X=x), W=w). \]
		\end{itemize}
		\end{teo}
		La interpretación de estos puntos, en palabras de \cite{pearl2009causality} se sigue intepretar la acción $do(X=x)$ como la imposición de un nuevo mecanismo el cual induce un sub-modelo caracterizado por $\mathcal{G}_{\bar{X}}$. El primer punto se obtiene al considerar que eliminar ecuaciones en un sistema no introduce nuevas dependencias y muestra que la d-separación es un criterio válido para la independencia condicional en el nuevo grafo $\mathcal{G}_{\bar{X}}$. El punto dos provee una condición para que una intervención tenga el mismo efecto que una observación pasiva y el tercer punto provee una condición para poder introducir, o eliminar una intervención externa sin afectar la probabilidad de observar $Y=y$.
		\begin{teo}{\cite{peters2017elements}}\\
		Los siguientes enunciados se cumplen:
		\begin{itemize}
		\item El cálculo do es completo; es decir, para cada intervención identificable existe una manera iterativa de aplicar las tres reglas que resulta en dicha intervención (\cite{huang2006pearl}, \cite{shpitser2006identification})
		\item Existe un algoritmo que es capaz de encontrar todas las intervenciones identificables (\cite{tian2002}, \cite{huang2006pearl})
		\item Existe un criterio necesario y suficiente para la identificabilidad de las distribuciones de intervención (\cite{shpitser2006identification}, \cite{huang2006pearl}).
		\end{itemize}
		\end{teo}
		Ademas
		\begin{cor} Solución al problema de la identificabilidad
		Una distribución $q=P(y_1,...,y_k | do(x_1),...,do(x_n))$ es identificable en un modelo gráfico causal $\mathcal{G}$ si existe una secuencia finita de transformaciones, donde cada una de las cuales corresponde a una de las reglas del teorema \ref{docalculus} que reduce $q$ a una expresión de probabilidad condicionada sólo en datos observacionales (i.e. no aparece el operador $do$)
		\end{cor}
	\subsection{Toma de decisiones}
		\subsubsection{Teorema de von Neumann-Morgenstern: existencia de utilidad}
		Sea $X$ un conjunto de \textit{alternativas}, los objetos a elegir son loterías de soprte finito; de manera formal, definimos
\[ L = \{ P:X \to [0,1] | \sum_{x \in X} P(x) = 1, \textrm{ $P$ tiene soporte finito} \}. \]
Una mezcla de loterías se puede definir de la siguiente manera: se toma una probabilidad $\alpha$ y se define una lotería $l^\alpha = \alpha P + (1- \alpha) Q$.\\
\\
Consideremos una relación  $\succeq L \times L$. El Teorema de von Neumann-Morgenstern requiere de los siguientes axiomas:
\begin{itemize}
\item A1. Órden débil: $\succeq$ es completa y transitiva
\item A2. Continuidad: Para toda $P,Q,R \in L$ tales que $P \succ Q \succ R$ existen $\alpha$ y $\beta$ tales que:
\[  \alpha P + (1-\alpha) R \succ Q \succ \beta P + (1-\beta) R\]
\item A3. Independencia: Para toda lotería $P,Q,R \in L$ y toda $\alpha \in (0,1)$ se tiene que $P \succeq Q$ si y sólo si $\alpha P + (1-\alpha) R \succeq \alpha Q  + (1-\alpha) R$.
\end{itemize}
\begin{teo}
Una relación $\succeq L \times L$ satisface los axiomas A1 - A3 si y sólo sí existe una función $u: X \to \mathbb{R}$ tal que para toda $P,Q \in L$ se tiene que $P \succeq Q$ si y sólo si 
\[ \sum_{x \in X} P(x) u(x) \geq \sum_{x \in X} Q(x) u(x).  \]
\end{teo}
Este Teorema dice que para un tomador de decisiones racional (es decir, que se comporta según los axiomas) existe una función de utilidad tal que las preferencias del tomador de decisiones son equivalentes a maximizar una utilidad esperada.
		\subsubsection{Teorema de DeFinetti: existencia de probabilidad subjetiva}
		De Finetti consideró apuestas sobre \textit{estados del mundo}. Consideremos que existen $1,...,n$ estados en el mundo. Una apuesta es una función que toma valores en el espacio de estados y los mapea a valores monetarios; es decir, una apuesta $x$ es un vector de $n$ elementos en donde $x_i$ es la ganancia en el caso de que la apuesta resulte en el estado $i$ como el ganador. Consideramos una relación $\succeq \subseteq X \times X$. Y consideremos los siguientes axiomas:
\begin{itemize}
\item A1. Órden débil: $\succeq$ es completa y transitiva.
\item A2. Continuidad: Para cada $x \in X$, los conjuntos $\{ y | x \succ y \}$ y $\{ y | y \succ x \}$ son abiertos.
\item A3. Aditividad: Para toda $x,y,z \in X$, $x \succeq y$ si y sólo si $x+z \succeq y+z$.
\item A4 Monotonicidad: Para toda $x,y \in X$, si se cumple que $x_i \geq y_i$ entonces se tiene que $x \succeq y$.
\item A5 No-trivialidad: Existen $x,y \in X$ tales que $x \succ y$.
\end{itemize}
Notemos que el axioma A3 sólo aplica si el tomador de decisiones es neutral al riesgo (\cite{gilboa2009decision}).
\begin{teo}
La relación $\succeq \subseteq X \times X$ satisface A1-A5 si y sólo si existe un vector de probabilidad $p$ de tamaño $n$ tal que para toda $x,y \in X$ se cumple que
\[ x \succeq y \textrm { si y sólo si } px \geq py. \]
\end{teo}
Entonces, la definición de probabilidad subjetiva sería aquella probabilidad utilizada por un tomador de decisiones para escoger una acción mediante el criterio de utilidad esperado.
		\subsubsection{Teorema de Savage: existencia de utilidad y probabilidad subjetiva}
	\subsection{Teoría de Juegos}
		\subsubsection{Juegos en forma normal}
		\subsubsection{Juegos en forma extensiva}
		\subsubsection{Representaciones más generales}
	
	\subsection{Trabajo Previo}

\section{Planteamiento y Modelado}

\section{Objetivos}
	\section{Objetivo General (debe ir relacionado con el título)}
	\section{Objetivos específicos}
	
\section{Metodología}
	\subsection{Hipótesis}
	\subsection{Metodología de Trabajo}

\section{Resultados Preliminares}
Para mostrar la factibilidad del modelo planteado, consideramos un escenario particular y tres casos por separado cuyo grado de dificultad es ascendente. El escenario es el de un médico que tiene que escoger uno de entre varios tratamientos para un paciente con enfermedad desconocida. Los tratamientos posibles conllevan riesgos y ventajas, que están regidos por un modelo causal.\\
\\
Los tres casos a considerar son los siguientes:\\
\\
En el primer caso, supondremos que el modelo causal es conocido completamente por el agente y veremos cuál sería su manera de actuar en este caso. Que el modelo sea conocido completamente significa que el agente conoce la estructura del grafo así como las probabilidades asociadas a cada variable.\\
\\ 
Posteriormente, en el segundo caso, el agente sólo tendrá a su disposición la estructura del grafo, pero no los parámetros de este. \\
\\
En el tercer caso, el agente no conocerá nada del modelo, pero tendrá \textit{creencias} sobre este; este caso generaliza los anteriores, pues en el primer y segundo caso las creencias con las que inicia el agente es la información que conoce sobre el modelo. En resumen, en los tres casos el agente contará con \textit{creencias} sobre el modelo causal verdadero; en el primer caso, sus creencias coincidirán plenamente con el modelo; en el segundo caso, sólo con la estructura y en el tercer caso, no tenemos garantía de nada. 
\subsection{Caso en el que se conoce el modelo completo}
Consideremos el caso en el que el modelo gráfico causal es completamente conocido por el tomador de decisiones, tanto en su estructura como en sus parámetros. En este caso, el juego consistiría en lo siguiente:\\
\begin{itemize}
\item Un modelo gráfico causal conocido para el tomador de decisiones.
\item Jugadores: Naturaleza y el Tomador de Decisiones.
\item Acciones: Las acciones de la Naturaleza consisten en asignar cierto estado incierto; en este caso, asigna inicialmente la enfermedad del paciente; posteriormente, selecciona sus acciones a partir del modelo causal.
\end{itemize}


\subsection{Caso en el que se conoce la estructura del modelo}
Aquí se conocen los efectos de las causas, pero no sus probabilidades, entonces primero debo conocer el subconjunto posible de consecuencias y ahí tal vez hacer exploración greedy.
\subsection{Caso general}
No tengo idea aun.
\section{Conclusiones}

\section{Extensiones posibles}
Más oponentes...la naturaleza tiene intenciones,

\bibliographystyle{apalike}
\bibliography{/Users/MauricioGS1/INAOE/Segundo_Semestre/Propuesta/Bibliografia.bib}
\end{document}
