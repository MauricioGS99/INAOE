\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amssymb,amsthm,amsmath,latexsym}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2018}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2018}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2018}

\begin{document}

\twocolumn[
\icmltitle{Playing against Nature: causal discovery for decision making under uncertainty}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2018
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Mauricio Gonzalez-Soto}{inaoe}
\icmlauthor{Luis Enrique Sucar}{inaoe}
\icmlauthor{Hugo Jair Escalante}{inaoe}

\end{icmlauthorlist}

\icmlaffiliation{inaoe}{Department of Computer Science, National Institute of Astrophysics Optics and Electronics (INAOE), Puebla, Mexico}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Causality, Causal Inference, Game Theory, Decision under Uncertainty}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
We consider decision problems under uncertainty where the decisions and the outcomes are related through a causal mechanism which is unknown to the decision maker. 
We study how a decision maker can learn about this causal mechanism through sequential decision making as well as using current causal knowledge inside each round in order to make better choices had she not considered causal knowledge.
\end{abstract}

\section{Introduction}
A fundamental part of intelligent reasoning is being able to make decisions under uncertain conditions (\cite{lake2017building}, \cite{danks2014unifying}, \cite{pearlwhy}). In some cases, a decision maker who faces an uncertain environment has enough information to make choices by maximizing expected utility, which is the formal criteria for making decisions if rational preferences are assumed (\cite{bernardo2000bayesian}, \cite{gilboa2009decision}). On the other hand, if enough information is not available, the decision maker could attempt to \textit{learn} from the environment by interacting with it.

Learning by interaction has been extensively studied by computer scientists using the Reinforcement Learning (RL) setting \cite{sutton1998reinforcement}, but the most common used techniques  in this field are purely associative and do not consider any high-level structure of the environment beyond what is expressable in a Markov Decision Process \cite{garnelo2016towards}.

Since human beings are known to learn causal graphical models in sequential decision making processes (\cite{sloman2006causal}, \cite{nichols2007decision}, \cite{meder2010observing}, \cite{hagmayer2013repeated}, \cite{danks2014unifying}), even though this learning is not perfect (\cite{rottman2014reasoning}), we propose that an autonomous agent can learn and use causal information while interacting with an uncertain environment which is governed by a fixed \textit{causal mechanism} which is unknown to the agent.  

The proposed way for an agent to learn from repeated interactions is by giving her \textit{beliefs} about the structure of the environment in form of a probability distribution over a suitable space. This beliefs will be updated according with what is observed.

While the standard setting in RL is to model the interaction agent-environment as an agent that moves from one \textit{state} to another inside a model of the environment and observing a reward as these transitions occur, we propose to model it as a \textit{game} between the decision maker and a player called \textit{Nature} which will select his actions from the causal model.

\section{Related Work}
Decision problems where the actions available to a decision maker are interventions over a known causal model are analyzed by \cite{lattimoreNIPS2016} as a \textit{bandit problem} where the optimal action must be learned over $T$ rounds of action-observation in which only one action can be chosen. In a classic bandit problem an agent chooses an \textit{arm} from a slot machine, observes a reward and then moves on to the next machine which is of the same kind and whose initial settings are independent of the previous machine and action \cite{sutton1998reinforcement}.

Several algorithms exists for finding the best arm in a multi-arm bandit, such as those described in \cite{bubeck2009pure}, \cite{audibert2010best}, \cite{gabillon2012best}, \cite{agarwal2014taming} , \cite{jamieson2014lil},  \cite{jamieson2014best},  \cite{ortega2014generalized}, \cite{chen2015optimal},\cite{carpentier2016tight},  \cite{russo2016simple},  \cite{kaufmann2016complexity}, but none of these works consider causal-governed environments.

As far as we know \cite{lattimoreNIPS2016} is the first paper to consider causal relations between the effects of actions. They consider a decision maker who must choose the best among several possible interventions on a given causal model. The optimality of the action in this context is in terms of the minimal regret. The case where the causal model is not known is left as future work.

By considering a causal model which is partially known and intervening variables from the unknown part of the model and by avoiding sampling arms that are considered sub-optimal, \cite{sen2017identifying} extend the work of \cite{lattimoreNIPS2016}.

The aforementioned papers assume the causal model is known to the decision makers so their work focuses on \textit{using} causal information to make good choices, but the problem of \textit{acquiring} this causal knowledge is left unatacked.

In this work we propose to acquire, by repeated interaction, causal information about the environment as well as using the current causal knowledge inside each round to make better decisions. By modelling the environment as a player in a game we allow it to have objectives to pursue which will allow to model a rich family of situations where several agents are competing against each other and a causal entity controls the outcomes. 
\section{Problem setup}{\label{problem_setup}}
By \textit{causality} we mean a stochastic binary relation between events of a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ denoted by $“\to”$ that is transitive, irreflexive and antisymmetric (\cite{spirtes2000causation}). 

A directed acyclic graph (DAG) can be used to represent all of the relations that occur in that space by considering a node for every variable that is related to another and a directed edge to express the causal relation, call this DAG $\mathcal{G}$ and consider a probability measure $P_{\mathcal{G}}$ that expresses the conditional statements from the DAG. 

We require that this measure satisfies the Markov Causal Condition, Causal Minimality and Causal Faithfulness as stated in \cite{spirtes2000causation}. The relation between $\mathbb{P}$ and $P_{\mathcal{G}}$ is given by the Manipulation Theorem of \cite{spirtes2000causation} and the Do-Calculus rules from \cite{pearl2009causality}. We also require that the condition known as Causal Sufficiency is satisfied by the model, which means there are not any causes lying \textit{outside} of the model. 

Consider a Decision Problem under Uncertainty $(A,E,C,\succeq)$ in which an agent has to choose one among several options $a \in A$ which are causally related to the elements of $C$. We assume that the decision maker has \textit{rational preferences}, and because of this we can substitute his preferences $\succeq$ for a utility function $u$. The decision maker does not know the probabilities nor the structure of the underlying causal model therefore he can not calculate the expected utility of any action. 

Instead, the agent will try to learn by interaction with the environment succesive rounds of decision making. Inside each round, any response from the environment will be independent from the previous rounds, but the actions of the decision maker, which will be based upon previously acquired causal information are expected to improve the utility for the agent.

We define a game between the decision maker and a new abstract player called Nature. The base game will be the original decision problem. Nature will be indifferent among the different possible outcomes of the game and will select its actions from the causal model. Nature having objectives to pursue (non-constant payments) will be left as future work.

\section{Belief formation and updating}
For an agent to reason about and modify his causal knowledge we endow her with a probability distribution $p(\theta)$ over a suitable space. Since we are considering causal graphical models, and for each DAG there exists a one-to-one correspondence to a matrix, one way to express beliefs about DAG's is using distributions over matrices for the structure of the graph.

Since the beliefs must behave as a \textit{local} model in a given moment, it is preferable to assign probabilities directly to causal models rather than edges in a model in order to have a fixed model when sampling from this probability assignement.

After each round of the base game, the probabilities representing causal beliefs will be updated in a Bayesian way in order to guarantee Bayesian consistency \cite{shoham2008multiagent}.

Consider the classical rain example from \cite{pearl2009causality} where a floor being wet causes it to be slippery and therefore that causes people to fall over it, but being wet can be caused by either a sprinkler turned on or because it recently rained, which itself is caused by the current season of the year. Suppose that a person who recently arrived in the city is about to go out. She doesn't know precisely which are the rainy months in this new place, but she believes that rainy dayws are almost the same that in her past home. She also noticed a nearby sprinkler which she believes is far away from her house to wet her floor. When she goes out and falls and notices the sprinkler turned on, that increases her belief about it affecting her floor. 

Even though she didn't knew the causal model itself, some days later she was certain that the sprinkler caused her floor to be wet and since the beliefs she had in her mind were about causal relations.

\section{Proposed Method}
In this section we describe our approach for studying decision making in causal environments as described in Section \ref{problem_setup}.

For the sake of explanation we consider three separate cases:

\begin{itemize}
\item The decision maker fully knows the causal model.
\item The decision maker knows only the structure of the causal model.
\item The decision maker does not know the causal model.
\end{itemize}

\subsection{The causal model is completely known}
Consider a decision problem under uncertainty where a decision maker has to choose on out of many elements of a set $A$ and where the consequence, or effect, of his actions is expressed by the realization of a random variable $Y$ which we will call \textit{target variable}.  The decision maker whishes to obtain a value from $Y$ that maximizes his utility. 

The relation between values of $Y$ and actions $a \in A$ is expressed through realizations of \textit{uncertain events}, which causally relate $a$ and values $y \in \textrm{Val}(Y)$ and this relations are expressed by a causal graphical model $\mathcal{G}$. The action chosen by the decision maker is considered as an \textit{intervention} over some variable in this causal model.

This is the simplest case of the three mentioned because if the decision maker fully knows the causal model, then she can proceed as in classic decision problems by directly obtaining the probabilities of different values for the target variable given that an intervention on \textit{treatment} is made and choose the treatment which achieves the highest probability for the desired value of the target variable will be chosen. The action selected will be a \textit{best response} for the decision maker as well as the maximum expected utility choice.

Pearl's do-calculus \cite{pearl2009causality} says that the effect of setting some variable $X_i$ to a value $x_j$ can be expressed in terms of observational distributions as follows:
\[ P(X_1,...,X_n | do(X_i = j )) = \prod_{k \neq i} P(X_k | \textrm{Pa}(X_k)) \] 

If a decision maker knows the causal model that is governing her environment, then she could use this expression to find out the probability of her desired value for the target variable and choose the action that maximizes this probability. 