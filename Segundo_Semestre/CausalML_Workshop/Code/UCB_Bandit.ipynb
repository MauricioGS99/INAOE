{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.datahubbs.com/multi-armed-bandits-reinforcement-learning-2/#Bandit-Problems\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ucb_bandit:\n",
    "    '''\n",
    "    Upper Confidence Bound k-bandit problem\n",
    "    \n",
    "    Inputs \n",
    "    ============================================\n",
    "    k: number of arms (int)\n",
    "    c:\n",
    "    iters: number of steps (int)\n",
    "    mu: set the average rewards for each of the k-arms.\n",
    "        Set to \"random\" for the rewards to be selected from\n",
    "        a normal distribution with mean = 0. \n",
    "        Set to \"sequence\" for the means to be ordered from \n",
    "        0 to k-1.\n",
    "        Pass a list or array of length = k for user-defined\n",
    "        values.\n",
    "    '''\n",
    "    def __init__(self, k, c, iters, mu='random'):\n",
    "        # Number of arms\n",
    "        self.k = k\n",
    "        # Exploration parameter\n",
    "        self.c = c\n",
    "        # Number of iterations\n",
    "        self.iters = iters\n",
    "        # Step count\n",
    "        self.n = 1\n",
    "        # Step count for each arm\n",
    "        self.k_n = np.ones(k)\n",
    "        # Total mean reward\n",
    "        self.mean_reward = 0\n",
    "        self.reward = np.zeros(iters)\n",
    "        # Mean reward for each arm\n",
    "        self.k_reward = np.zeros(k)\n",
    "        \n",
    "        if type(mu) == list or type(mu).__module__ == np.__name__:\n",
    "            # User-defined averages            \n",
    "            self.mu = np.array(mu)\n",
    "        elif mu == 'random':\n",
    "            # Draw means from probability distribution\n",
    "            self.mu = np.random.normal(0, 1, k)\n",
    "        elif mu == 'sequence':\n",
    "            # Increase the mean for each arm by one\n",
    "            self.mu = np.linspace(0, k-1, k)\n",
    "        \n",
    "    def pull(self):\n",
    "        # Select action according to UCB Criteria\n",
    "        a = np.argmax(self.k_reward + self.c * np.sqrt(\n",
    "                (np.log(self.n)) / self.k_n))\n",
    "            \n",
    "        reward = np.random.normal(self.mu[a], 1)\n",
    "        \n",
    "        # Update counts\n",
    "        self.n += 1\n",
    "        self.k_n[a] += 1\n",
    "        \n",
    "        # Update total\n",
    "        self.mean_reward = self.mean_reward + (\n",
    "            reward - self.mean_reward) / self.n\n",
    "        \n",
    "        # Update results for a_k\n",
    "        self.k_reward[a] = self.k_reward[a] + (\n",
    "            reward - self.k_reward[a]) / self.k_n[a]\n",
    "        \n",
    "    def run(self):\n",
    "        for i in range(self.iters):\n",
    "            self.pull()\n",
    "            self.reward[i] = self.mean_reward\n",
    "            \n",
    "    def reset(self, mu=None):\n",
    "        # Resets results while keeping settings\n",
    "        self.n = 1\n",
    "        self.k_n = np.ones(self.k)\n",
    "        self.mean_reward = 0\n",
    "        self.reward = np.zeros(iters)\n",
    "        self.k_reward = np.zeros(self.k)\n",
    "        if mu == 'random':\n",
    "            self.mu = np.random.normal(0, 1, self.k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "iters = 1000\n",
    "\n",
    "ucb_rewards = np.zeros(iters)\n",
    "# Initialize bandits\n",
    "ucb = ucb_bandit(k, 2, iters)\n",
    "\n",
    "episodes = 1000\n",
    "# Run experiments\n",
    "for i in range(episodes): \n",
    "    ucb.reset('random')\n",
    "    # Run experiments\n",
    "    ucb.run()\n",
    "    \n",
    "    # Update long-term averages\n",
    "    ucb_rewards = ucb_rewards + (\n",
    "        ucb.reward - ucb_rewards) / (i + 1)\n",
    "    \n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(ucb_rewards, label=\"UCB\")\n",
    "plt.legend(bbox_to_anchor=(1.2, 0.5))\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.title(\"Average UCB Rewards after \" \n",
    "          + str(episodes) + \" Episodes\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
