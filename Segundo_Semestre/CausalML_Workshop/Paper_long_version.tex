\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amssymb,amsthm,amsmath,latexsym}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2018}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2018}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2018}

\begin{document}

\twocolumn[
\icmltitle{Playing against Nature: causal discovery for decision making under uncertainty}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2018
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Mauricio Gonzalez-Soto}{inaoe}
\icmlauthor{Luis Enrique Sucar}{inaoe}
\icmlauthor{Hugo Jair Escalante}{inaoe}

\end{icmlauthorlist}

\icmlaffiliation{inaoe}{Department of Computer Science, National Institute of Astrophysics Optics and Electronics (INAOE), Puebla, Mexico}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Causality, Causal Inference, Game Theory, Decision under Uncertainty}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
We consider decision problems under uncertainty $(A,E,C,u)$ where elements in $C$ and elements in $E$ are related through a causal mechanism which is unknown to the decision maker. 
We study how a decision maker can learn about this causal mechanism through sequential decision making as well as using current causal knowledge inside each round in order to make better choices had she not considered causal knowledge.
\end{abstract}

\section{Introduction}
A fundamental part of intelligent reasoning is being able to make decisions under uncertain conditions (\cite{lake2017building}, \cite{danks2014unifying}, \cite{pearlwhy}). In some cases, a decision maker who faces an uncertain environment has enough information to make choices by maximizing expected utility, which is the formal criteria for making decisions if rational preferences are assumed (\cite{bernardo2000bayesian}, \cite{gilboa2009decision}). On the other hand, if enough information is not available, the decision maker could attempt to \textit{learn} from the environment by interacting with it.

Learning by interaction has been extensively studied by computer scientists using the Reinforcement Learning (RL) setting \cite{sutton1998reinforcement}, but the most common used techniques  in this field are purely associative and do not consider any high-level structure of the environment beyond what is expressable in a Markov Decision Process \cite{garnelo2016towards}.

Since human beings are known to learn causal graphical models in sequential decision making processes (\cite{sloman2006causal}, \cite{nichols2007decision}, \cite{meder2010observing}, \cite{hagmayer2013repeated}, \cite{danks2014unifying}), although this learning is not perfect (\cite{rottman2014reasoning}), we propose that an autonomous agent can learn and use causal information while interacting with an uncertain environment which is governed by a fixed \textit{causal mechanism} which is unknown to the agent.  

The proposed way for an agent to learn from repeated interactions is by giving her \textit{beliefs} about the structure of the environment in form of a probability distribution over a suitable space. This beliefs will be updated according with what is observed.

While the standard setting in RL is to model the interaction agent-environment as an agent moving from one \textit{state} to another inside a model of the environemtn and observing a reward as this transitions occur, we propose to model it as a \textit{game} between the decision maker and a player called Nature which will select his actions from the causal model.

\section{Related Work}
Decision problems where the actions available to a decision maker are interventions over a known causal model are analyzed by \cite{lattimoreNIPS2016} as a \textit{bandit problem} where the optimal action must be learned over $T$ rounds of action-observation in which only one action can be chosen. In a classic bandit problem an agent chooses an \textit{arm} from a slot machine, observes a reward and then moves on to the next machine which is of the same kind and whose initial settings are independent of the previous machine and action \cite{sutton1998reinforcement}.

Several algorithms exists for finding the best arm in a multi-arm bandit, such as \cite{bubeck2009pure}, \cite{audibert2010best}, \cite{gabillon2012best}, \cite{agarwal2014taming} , \cite{jamieson2014lil},  \cite{jamieson2014best},  \cite{ortega2014generalized}, \cite{chen2015optimal},\cite{carpentier2016tight},  \cite{russo2016simple},  \cite{kaufmann2016complexity}, but none of this works consider causal-governed environments.

Up to our current knowledge \cite{lattimoreNIPS2016} is the first paper to consider causal relations between the effects of actions. They consider a decision maker who must choose the best among several possible interventions on a given, and fully known, causal model. The optimality of the action in this context is in terms of the minimal regret. The case where the causal model is not known is left as future work.

By considering a causal model which is partially known and intervening variables from the unknown part of the model and by avoiding sampling arms that are considered sub-optimal \cite{sen2017identifying} extend the work of \cite{lattimoreNIPS2016}.

The aforementioned papers assume the causal model is known to the decision makers so their work focuses on \textit{using} causal information to make good choices but the problem of \textit{acquiring} this causal knowledge is left unatacked.

In this work we propose to acquire, by repeated interaction, causal information about the environment as well as using the current causal knowledge inside each round to make better decisions. By modelling the environment as a player in a game we allow it to have objectives to pursue which will allow to model a rich family of situations where several agents are competing against each other and a causal entity controls the outcomes. 
\section{Problem setup}
By \textit{causality} we mean a stochastic binary relation between events of a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ denoted by $“\to”$ that is transitive, irreflexive and antisymmetric (\cite{spirtes2000causation}). 

A directed acyclic graph (DAG) can be used to represent all of the relations that occur in that space by considering a node for every variable that is related to another and a directed edge to express the causal relation, call this DAG $\mathcal{G}$ and consider a probability measure $P_{\mathcal{G}}$ that expresses the conditional statements from the DAG. 

We require that this measure satisfies the Markov Causal Condition, Causal Minimality and Causal Faithfulness as stated in \cite{spirtes2000causation}. The relation between $\mathbb{P}$ and $P_{\mathcal{G}}$ is given by the Manipulation Theorem of \cite{spirtes2000causation} and the Do-Calculus rules from \cite{pearl2009causality}. We also require that the condition known as Causal Sufficiency is satisfied by the model, which means there are not any causes lying \textit{outside} of the model. 

Consider a Decision Problem under Uncertainty $(A,E,C,\succeq)$ where family of uncertain events $E$ and the set of consequences $C$ are causally related. We assume that the decision maker has \textit{rational preferences}, and because of this we can substitute his preferences $\succeq$ for a utility function $u$. The decision maker does not know the probabilities nor the structure of the underlying causal model therefore he can not calculate the expected utility of any action. 

Instead, the agent will try to learn by interaction with the environment succesive rounds of decision making. Inside each round, any response from the environment will be independent from the previous rounds, but the actions of the decision maker, which will be based upon previously acquired causal information are expected to improve the utility for the agent.

We define a game between the decision maker and a new abstract player called Nature. The base game will be the original decision problem.

Nature will be indifferent among the different possible outcomes of the game and will select its actions from the causal model. Nature having objectives to pursue (non-constant payments) will be left as future work.

\section{Belief formation and updating}
For an agent to reason about and modify his causal knowledge we endow her with a probability distribution $p(\theta)$ over a suitable space. Since we are considering causal graphical models, and for each DAG there exists a one-to-one correspondence to a matrix, one way to express beliefs about DAG's is using distributions over matrices for the structure of the graph.

Since the beliefs must behave as a \textit{local} model in a given moment, it is preferable to assign probabilities directly to causal models rather than edges in a model in order to have a fixed model when sampling from this probability assignement.

After each round of the base game, the probabilities representing causal beliefs will be updated in a Bayesian way in order to guarantee Bayesian consistency \cite{shoham2008multiagent}.

Consider the classical rain example from \cite{pearl2009causality} where a floor being wet causes it to be slippery and therefore that causes people to fall over it, but being wet can be caused by either a sprinkler turned on or because it recently rained, which itself is caused by the current season of the year. Suppose that a person who recently arrived in the city is about to go out. She doesn't know precisely which are the rainy months in this new place, but she believes that rainy dayws are almost the same that in her past home. She also noticed a nearby sprinkler which she believes is far away from her house to wet her floor. When she goes out and falls and notices the sprinkler turned on, that increases her belief about it affecting her floor. 

Even though she didn't knew the causal model itself, some days later she was certain that the sprinkler caused her floor to be wet and since the beliefs she had in her mind were about causal relations.
\section{Test scenario}
Consider a patient who arrives at a hospital and he can have either disease $A$ or $B$. The doctor can either give him some pill or send him into surgery.  Both treatments entail risks and whether the treatment cures the patient or not depends on which disease it had originally. The causal model that governs this situation is shown in Figure \ref{causal_model}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{/Users/MauricioGS1/INAOE/Segundo_Semestre/Seminario/Semana12/codigo/causal_graph.png}}
\caption{Causal graphical model for the test scenario: the target variable \textit{Lives} is causally influenced by the disease the patient has, the treatment assigned and the survival to the secondary effects of treatment.}
\label{causal_model}
\end{center}
\vskip -0.2in
\end{figure}

The node corresponding to the variable lives is the \textit{target variable} and, in this example, the only variable that can be intervened upon is the treatment.

In this scenario, Nature's move will consist in randomly assigning the disease to the patient. Then, the medic will asign a treatment, the decision nodes for this play form an information set because the medic doesn't know how she arrived there since she doesn't know what disease did Nature assign. Finally, Nature will sample the consequence of the treatment from the causal model and the medic will observe the outcome.

For the sake of explanation we consider three separate cases:
\begin{itemize}
\item The decision maker fully knows the causal model.
\item The decision maker knows only the structure of the causal model.
\item The decision maker does not know the causal model.
\end{itemize}

\subsection{The casual model is completely known}
Consider a decision problem under uncertainty where a decision maker has to choose on out of many elements of a set $A$ and where the consequence, or effect, of his actions is expressed by the realization of a random variable $Y$. The decision maker whishes to obtain a value from $Y$ that maximizes his utility. 

The relation between values of $Y$ and actions $a \in A$ is expressed through realizations of \textit{uncertain events}, which causally relate $a$ and values $y \in \textrm{Val}(Y)$ and this relations are expressed by a causal graphical model $\mathcal{G}$.

This is the simplest case of the three mentioned because if the decision maker fully knows the causal model, then she can proceed as in classic decision problems by directly obtain the probabilities of different values for the target variable given that an intervention on \textit{treatment} is made and choose the treatment which achieves the highest probability for the desired value of the target variable will be chosen. The action selected will be a \textit{best response} for the decision maker as well as the maximum expected utility choice.

Pearl's  do-calculus \cite{pearl2009causality} says that the effect of setting some variable $X_i$ to a value $x_j$ can be expressed in terms of observational distributions as follows:
\[ P(X_1,...,X_n | do(X_i = j )) = \prod_{k \neq i} P(X_k | \textrm{Pa}(X_k)) \] 

For the test scenario whose causal graphical model is shown in Figure \ref{causal_model}, we see  by applying the previous formula that the interventional distribution $P_{do(Tr)}(V)$ is given by
\[ P(V | do(Tr))=P(Y | E, Tr, R)P(R | Tr) P(E). \]

In fact, noting that the variable to be intervened by the decision maker doesn't has parents in the graph one can explicitly calculate the probability of the target variable given a value for the treatment using variable elimination (\cite{koller2009probabilistic}) given the treatment as evidence. 

On the other hand, if a decision maker hadn't known the causal nature of the problem, but had he known the probabilities of the possible outomes given his own actions then a choice could be made using expected utilities. 

What is shown with this \textit{gedankenexperiment} is that in this full knowledge setting causal information achieves the same performance than simple probabilities.

\subsection{Only the structure is known}
If only the structure of the graph is known, then it is not obvious how to find the best action to make since the information required for calculating expected utilities is not available.

The decision maker will attempt to learn from her uncertain environment by forming \textit{beliefs} over the unknown parameters (conditional probabilities) of the model and update them according to the observed outcomes. 

We consider a repeated game in which the base game is described by the narrative of the test scenario: 
\begin{itemize}
\item \textbf{Players:} The set of players of this game is the set whose elements are the original decision maker, and a new player called Nature.
\item \textbf{Actions:} The actions for the decision maker are the possible treatments that the medic can give to patient. And the actions for the nature are the set of possible diseases, and the set of outcomes for the sick patient.
\item \textbf{Preferences:} The decision maker satisfies the von Neumann-Morgenstern axioms of rationality and he receives a payment of $1$ if the patient lives and $0$ in other case. Nature is indifferent over outcomes.
\item \textbf{Beliefs:} Since the decision maker doesn't know some aspects about her environment, she will encode any uncertainty about it in a probability distribution $p(\theta)$.
\end{itemize}

Notice that the base game is an extensive game with imperfect information since the decision maker makes a choice without knowing the play made by Nature, which is the asigning a disease to the patient, and then assigning the outcome for the patient after the medic has chosen a treatment. 

Since she knows the graph structure, she can explicitly find a non interventional expression for the interventional distribution and update her beliefs about this unknown quantities. If the decision maker were not allowed to know, at the end of each round, the play of the Nature then this will have to be estimated as a hidden variable using, for example, the EM algorithm \cite{dempster1977maximum} but meanwhile we are assuming that this information is available at the end of each round.

Given the structure of the model, which is shown in Figure \ref{causal_model}, we see that the involved probabilities in any calculations are:
\[ P(\textrm{disease}), P(\textrm{treatment}), P(\textrm{reaction} | \textrm{treatment}), \]
\[P(\textrm{lives} | \textrm{disease, treatment, reaction}). \]
denoting the variables in the model as $X_1 = \textrm{disease}$, $\textrm{treatment}=X_2$, $\textrm{reaction}=X_3$ and $\textrm{lives}=Y$ then a natural notation for the probabilities arises as following:
\[ p_1 = P(\textrm{disease}), \]
\[ p_2 = P(\textrm{treatment}), \]
\[ p_3 = P(\textrm{reaction}),\]
\[p_y= P(\textrm{lives}), \]
\[ p_{3 | 2} = P(\textrm{reaction} | \textrm{treatment}), \]
\[p_{y |1,2,3} = P(\textrm{lives} | \textrm{disease, treatment, reaction}). \]

Since these distributions fully characterize the model, the decision maker will have beliefs over each one of this parameters. 

Notice that each of this parameter is itself a distribution of length equal to the number of possible values of the variable which is being conditioned, call the maximum number of possible values $k$ 

A distribution suitable to modelling discrete probability vectors is the $k$-dimensional Dirichlet distribution, whose support is the set of probability vectors of length $k$ \cite{hjort2010bayesian}. 

The $k$ dimensional Dirichlet distribution has a density $f$ with respect to the Lebesgue measure given by

\[ \frac{1}{B(\alpha)}  \prod_{i=1}^k x_i^{\alpha-1},\]

where $(x_1,...,x_k)$ are such that $\sum_{i=1}^k x_i =1$ and $\alpha=(\alpha_1,...,\alpha_k)$

The Dirichlet distribution is useful and practical since it is conjugate for itself \cite{bernardo2000bayesian}.

So, in this way, the decision maker will have beliefs about the CPT's. Using the current beliefs, a causal graphical model can be specified. Using this fully specified (structure + parameters), the decision maker will make her choice as in Case 1.

Previously we argued that the agent's beliefs were going to be \textit{distributions} over a suitable space, but what is going to be updated are the parameters of such distributions. Namely, the $\alpha$ corresponding to the Dirichlet random variable assigned to each CPT.

For the belief updating, given a new data point,  two cases must be considered:
\begin{itemize}
\item The variable to update has no parents.
\item The variable to update has parents.
\end{itemize}

In the first case, if a prior Dirichlet($\alpha$) is used, then the posterior is given by
\[ \textrm{Dirichlet}(\alpha + c) \]
where $c$ is a vector of the number of occurrences of that observed data point. 

For the second case, we must consider both the occurrences of that data point as well as the parents for each of the variables. Following \cite{barber2012bayesian} we denote as $\theta_i(X,j)$ the probability $P(X=i | Pa(X)=j)$ In this case, if the prior of $X$ is given by a a Dirichlet($\alpha$), then the posterior for a variable $X$ with parents $Pa(X)$ and an observed data point is given by 
\[ \prod_j \textrm{Dirichlet}(\alpha + u(j)) \]
where $u(j)$ counts the number of times that in the data point the parents of $X$ had the value $j$.

\subsubsection{Experiment}
As proof of concept we implemented this belief updating. We started with a random assignation of the $\alpha$ parameter for each of the distributions considered. 

With this parameters, the decision maker forms a causal model and chooses the action that maximizes the probability of the desired value for target variable. In the medic scenario, this desired value corresponds to the patient living. With this action chosen, we simulate from the causal graphical model using the chosen action as evidence. This evidence is used to update the parameters, which then will be used to generate a new causal model, and so on.

If we plot the average rewards obtained we observe they converge to 1 as shown in the next figure.


\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{/Users/MauricioGS1/INAOE/Segundo_Semestre/CausalML_Workshop/rewar.png}}
\caption{Average reward obtained by the decision maker in 50 rounds of the decision makin-belief updating cycle.}
\label{average_reward}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{The model is not known}
The causal model were fully unknown, the decision maker will have to deal with the problem using only any previous knowledge and his own intuitions. Again, consider the set of any previous knowledge and considerations will as \textit{beliefs} about the uncertainties in the environment, which will take the form of a probability distributions over a suitable space. 

For this third case, we make an extra assumption: the maximum number of variables in the model is known to the decision maker. This allow to consider only distributions of fixed dimension. In this case we will consider distributions over matrixes of fixed size rather than vectors as in the previous case. 

As in the previous case, consider a repeated game where the base game consists of Nature assigning a disease to a patient, a medic choosing a treatment for it and Nature once again choosing the final outcome. This final choice of Nature is governed by the causal model.

In this game, as well as in the previous one, the decision maker will attempt to learn by updating, and using, beliefs in a suitable way. 

The most notable difference with the previous case is that the \textit{structure} of the model is also to be learned in such a way that both the structure and parameters converge to the true model in the limit. 

In the previous case the decision maker knew the form of the CPT's involved in any calculation. In this case, she doesn't know the structure of the DAG so which CPT's are involved is unknown.

If the decision maker knew exactly what variables are involved, even though she didn't know how they are connected, she could use a \textit{Dirichlet Process} to generate Dirichlet distributions and generate causal graphical models the same way as in case 2. The Dirichlet Process, which was introduced by \cite{ferguson1973bayesian}, is a stochastic process whose finite-dimensional distributions are Dirichlet random variables \cite{hjort2010bayesian}, \cite{ghosal2017fundamentals}. It is the most common \textit{nonparametric prior} in the context of Bayesian Nonparametrics.

Belief updating in this context is yet to be studied.


 
 
%A natural way of encoding beliefs about causal relations is to consider all the possible variables and assign probabilities to the edges that connect them in a Directed Aciclyc Graph (DAG). Since the decision maker pretends to \textit{use} her beliefs and not only update them it is necessary that in each round of the game she can use her current knowledge in order to choose an action.

%Had she a DAG in mind whose edges have associated weights, she could sample a random variable with a $U(0,1)$ distribution and see if that sample value is less than the probability attached to the edge, if so, then she fixes this edge in the model. Proceeding this way, she will have the structure of causal graphical model, which will be then used as in Case 2.


\bibliography{/Users/MauricioGS1/INAOE/Segundo_Semestre/Propuesta/Bibliografia.bib}
\bibliographystyle{icml2018}

\end{document}