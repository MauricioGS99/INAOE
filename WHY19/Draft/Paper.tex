\def\year{2018}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai18}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
\pdfinfo{
/Title (A Guiding Principle for Causal Decision Problems)
/Author (M. Gonzalez-Soto, L.E. Sucar, H.J. Escalante)
}

\title{A Guiding Principle for Causal Decision Problems}
\author{M. Gonzalez-Soto, L.E. Sucar, H.J. Escalante\\
National Institute of Astrophysics Optics and Electronics (INAOE)\\
Luis Enrique Erro No. 1, Santa Maria Tonanzintla, Puebla, Mexico.
}

\begin{document}
\maketitle
\begin{abstract}
We define a Causal Decision Problem as a Decision Problem where available actions and outcomes are connected through the variables of a Causal Graphical Model $\mathcal{G}$ and propose a solution criteria based on Pearl's Do-Calculus and the Expected Utility criteria for rational preferences. The implementation of this criteria leads to an on-line decision making procedure that has been shown to have similar performance to classic Reinforcement Learning algorithms while allowing for a causal model of an environment to be learned. 
\end{abstract}

\section{Introduction}
\noindent

\section{Rationality and Expected Utility}
Rationality in a Decision Making setting is defined axiomatically in a way that the \textit{preferences} of a decision maker are logically consistent. If rational preferences are assumed, then it is known that the coherent criteria for making choices is the maximization of expected utility, either with respect to a known utility function and probability distribution or a pair of \textit{subjective} objects (\cite{bernardo2000bayesian}, \cite{gilboa2009decision}). Rational decision making has been the standard theory in economics both as a \textit{normative} and a \textit{descriptive} theory of human behavior and it has been the subject of multiple debates (\cite{tversky1974judgment}, \cite{kahneman1982judgment}). In this work we pretend to take a \textit{normative} view for a rational decision maker who faces an uncertain environment which is controlled by some unknown causal mechanism.
\section{Optimal Policies}
Given a Reinforcement Learning Problem defined over a Markov Decision Process, a \textit{policy} is a function from the space of \textit{states} to the space of \textit{actions} which is interpreted as what should an agent do in a given state (\cite{sutton1998reinforcement}). An \textit{optimal policy} is a policy which is optimal in the sense of achieiving the maximum possible reward, which is equivalent to finding the optimal action in the sense of the maximum expected utility (\cite{webb2007game}).
\section{Causal Decision Problems}
We define a Causal Decision Problem under Uncertainty as a tuple $(\mathcal{A}, \mathcal{E}, \mathcal{C}, \mathcal{G}, \succeq)$ where  $(\mathcal{A}, \mathcal{E}, \mathcal{C}, \succeq)$ is a classical Decision Problem under Uncertainty (\cite{bernardo2000bayesian} and $\mathcal{G}$ is a Causal Graphical Model (\cite{koller2009probabilistic}, \cite{sucar2015probabilistic}) such that the set of available actions $\mathcal{A}$ and the set of outcomes $\mathcal{C}$ are related through the variables of the Causal Model $\mathcal{G}$; i.e., the events in the family $\mathcal{E}$ correspond to variables in $\mathcal{G}$. It is assumed that the agent does not know the Causal Model $\mathcal{G}$, which is equivalent to not knowing the probabilities of the events $E \in \mathcal{E}$. The model $\mathcal{G}$ is also assumed to remain fixed and to be invariant under interventions (\cite{woodward2005making}) and to satisfy the conditions expressed in \cite{spirtes2000causation}. The variable in $\mathcal{G}$ which encodes the consequence of the action taken by the agent will be referred as \textit{target variable} since it is the variable where the agent whishes to obtain a desired result. 
\section{Proposed Solution}
Since rationality is assumed we must seek to maximize the expected utility of the agent in terms of his current knowledge, which is expressed as a (subjective) probability distribution. Using the awareness of the agent about a causal mechanism governing the environment, intuition encourages to use this causal relations to cause some desirable action, as expressed by \cite{joyce1999foundations}

Consider a Causal Decision Problem as stated above where the target variable, call it $Y$, takes its values in the set $\{0,1\}$ and without loss of generality assume that $1$ is the desired output for the agent, then he must choose the action $a^\ast \in \mathcal{A}$ such that
\[ P(Y=1 | do(a^\ast)) \geq P(Y=1 | do(a^\ast)) \textrm{ for all } a \in \mathcal{A}. \]
Since the action chosen is the action with the highest probability of producing the most desired action, then it is the action that maximizes the expected utility for the agent. If an action $a_0 \in \mathcal{A}$ yielded a higher expected utility, that would imply that it has higher probability of \textit{causing} the same desired action, but this is not possible because of how $a^\ast$ is obtained.

\section{On-line decision making and causal learning}
In \cite{gonzalez2018playing} a decision-making procedure was proposed using the Proposed Solution together with Bayesian \textit{belief updating} procedure in order to learn an optimal action while acquiring a causal model in the environment, this was applied in the simpler case where the decision maker knows the \textit{structure} of the model $\textit{G}$. In the referred work, a decision maker held \textit{beliefs} about the causal information of the environment, which were encoded into probability distributions. Those beliefs were used \textit{as if} they were the true model governing the environment in order to choose an action according to the Solution proposed here. Beliefs were updated in a Bayesian fashion after observing the outcome. 

The actions learned after a series of decision rounds obtained similar performance (in terms of average reward) as an agent learning using the classical Q-Learning procedure. Experimentally, this shows that causal information allows an agent to learn an optimal action, in the sense of expected utility, as well as learning a causal model of the environment.
\bibliography{Bibliografia.bib}
\bibliographystyle{aaai}
\end{document}